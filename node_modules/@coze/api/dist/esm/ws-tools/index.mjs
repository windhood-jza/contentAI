import * as __WEBPACK_EXTERNAL_MODULE_uuid__ from "uuid";
import * as __WEBPACK_EXTERNAL_MODULE_axios__ from "axios";
import * as __WEBPACK_EXTERNAL_MODULE_ws__ from "ws";
import * as __WEBPACK_EXTERNAL_MODULE_reconnecting_websocket__ from "reconnecting-websocket";
import * as __WEBPACK_EXTERNAL_MODULE_os__ from "os";
import * as __WEBPACK_EXTERNAL_MODULE_node_fetch__ from "node-fetch";
import "crypto";
import "jsonwebtoken";
const StreamProcessorWorklet = `
class StreamProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.hasStarted = false;
    this.hasInterrupted = false;
    this.outputBuffers = [];
    this.bufferLength = 128;
    this.write = { buffer: new Float32Array(this.bufferLength), trackId: null };
    this.writeOffset = 0;
    this.trackSampleOffsets = {};
    this.port.onmessage = (event) => {
      if (event.data) {
        const payload = event.data;
        if (payload.event === 'write') {
          const int16Array = payload.buffer;
          const float32Array = new Float32Array(int16Array.length);
          for (let i = 0; i < int16Array.length; i++) {
            float32Array[i] = int16Array[i] / 0x8000; // Convert Int16 to Float32
          }
          this.writeData(float32Array, payload.trackId);
        } else if (
          payload.event === 'offset' ||
          payload.event === 'interrupt'
        ) {
          const requestId = payload.requestId;
          const trackId = this.write.trackId;
          const offset = this.trackSampleOffsets[trackId] || 0;
          this.port.postMessage({
            event: 'offset',
            requestId,
            trackId,
            offset,
          });
          if (payload.event === 'interrupt') {
            this.hasInterrupted = true;
          }
        } else {
          throw new Error(\`Unhandled event "\${payload.event}"\`);
        }
      }
    };
  }

  writeData(float32Array, trackId = null) {
    let { buffer } = this.write;
    let offset = this.writeOffset;
    for (let i = 0; i < float32Array.length; i++) {
      buffer[offset++] = float32Array[i];
      if (offset >= buffer.length) {
        this.outputBuffers.push(this.write);
        this.write = { buffer: new Float32Array(this.bufferLength), trackId };
        buffer = this.write.buffer;
        offset = 0;
      }
    }
    this.writeOffset = offset;
    return true;
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];
    const outputChannelData = output[0];
    const outputBuffers = this.outputBuffers;
    if (this.hasInterrupted) {
      this.port.postMessage({ event: 'stop' });
      return false;
    } else if (outputBuffers.length) {
      this.hasStarted = true;
      const { buffer, trackId } = outputBuffers.shift();
      for (let i = 0; i < outputChannelData.length; i++) {
        outputChannelData[i] = buffer[i] || 0;
      }
      if (trackId) {
        this.trackSampleOffsets[trackId] =
          this.trackSampleOffsets[trackId] || 0;
        this.trackSampleOffsets[trackId] += buffer.length;
      }
      return true;
    } else if (this.hasStarted) {
      this.port.postMessage({ event: 'stop' });
      return false;
    } else {
      return true;
    }
  }
}

registerProcessor('stream_processor', StreamProcessor);
`;
const script = new Blob([
    StreamProcessorWorklet
], {
    type: 'application/javascript'
});
const src = URL.createObjectURL(script);
const StreamProcessorSrc = src;
/**
 * Constants for help with visualization
 * Helps map frequency ranges from Fast Fourier Transform
 * to human-interpretable ranges, notably music ranges and
 * human vocal ranges.
 */ // Eighth octave frequencies
const octave8Frequencies = [
    4186.01,
    4434.92,
    4698.63,
    4978.03,
    5274.04,
    5587.65,
    5919.91,
    6271.93,
    6644.88,
    7040.0,
    7458.62,
    7902.13
];
// Labels for each of the above frequencies
const octave8FrequencyLabels = [
    'C',
    'C#',
    'D',
    'D#',
    'E',
    'F',
    'F#',
    'G',
    'G#',
    'A',
    'A#',
    'B'
];
/**
 * All note frequencies from 1st to 8th octave
 * in format "A#8" (A#, 8th octave)
 */ const noteFrequencies = [];
const noteFrequencyLabels = [];
for(let i = 1; i <= 8; i++)for(let f = 0; f < octave8Frequencies.length; f++){
    const freq = octave8Frequencies[f];
    noteFrequencies.push(freq / Math.pow(2, 8 - i));
    noteFrequencyLabels.push(octave8FrequencyLabels[f] + i);
}
/**
 * Subset of the note frequencies between 32 and 2000 Hz
 * 6 octave range: C1 to B6
 */ const voiceFrequencyRange = [
    32.0,
    2000.0
];
const voiceFrequencies = noteFrequencies.filter((_, i)=>noteFrequencies[i] > voiceFrequencyRange[0] && noteFrequencies[i] < voiceFrequencyRange[1]);
const voiceFrequencyLabels = noteFrequencyLabels.filter((_, i)=>noteFrequencies[i] > voiceFrequencyRange[0] && noteFrequencies[i] < voiceFrequencyRange[1]);
/**
 * Output of AudioAnalysis for the frequency domain of the audio
 * @typedef {Object} AudioAnalysisOutputType
 * @property {Float32Array} values Amplitude of this frequency between {0, 1} inclusive
 * @property {number[]} frequencies Raw frequency bucket values
 * @property {string[]} labels Labels for the frequency bucket values
 */ /**
 * Analyzes audio for visual output
 * @class
 */ class AudioAnalysis {
    /**
   * Retrieves frequency domain data from an AnalyserNode adjusted to a decibel range
   * returns human-readable formatting and labels
   * @param {AnalyserNode} analyser
   * @param {number} sampleRate
   * @param {Float32Array} [fftResult]
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {AudioAnalysisOutputType}
   */ static getFrequencies(analyser, sampleRate, fftResult) {
        let analysisType = arguments.length > 3 && void 0 !== arguments[3] ? arguments[3] : 'frequency', minDecibels = arguments.length > 4 && void 0 !== arguments[4] ? arguments[4] : -100, maxDecibels = arguments.length > 5 && void 0 !== arguments[5] ? arguments[5] : -30;
        if (!fftResult) {
            fftResult = new Float32Array(analyser.frequencyBinCount);
            analyser.getFloatFrequencyData(fftResult);
        }
        const nyquistFrequency = sampleRate / 2;
        const frequencyStep = 1 / fftResult.length * nyquistFrequency;
        let outputValues;
        let frequencies;
        let labels;
        if ('music' === analysisType || 'voice' === analysisType) {
            const useFrequencies = 'voice' === analysisType ? voiceFrequencies : noteFrequencies;
            const aggregateOutput = Array(useFrequencies.length).fill(minDecibels);
            for(let i = 0; i < fftResult.length; i++){
                const frequency = i * frequencyStep;
                const amplitude = fftResult[i];
                for(let n = useFrequencies.length - 1; n >= 0; n--)if (frequency > useFrequencies[n]) {
                    aggregateOutput[n] = Math.max(aggregateOutput[n], amplitude);
                    break;
                }
            }
            outputValues = aggregateOutput;
            frequencies = 'voice' === analysisType ? voiceFrequencies : noteFrequencies;
            labels = 'voice' === analysisType ? voiceFrequencyLabels : noteFrequencyLabels;
        } else {
            outputValues = Array.from(fftResult);
            frequencies = outputValues.map((_, i)=>frequencyStep * i);
            labels = frequencies.map((f)=>`${f.toFixed(2)} Hz`);
        }
        // We normalize to {0, 1}
        const normalizedOutput = outputValues.map((v)=>Math.max(0, Math.min((v - minDecibels) / (maxDecibels - minDecibels), 1)));
        const values = new Float32Array(normalizedOutput);
        return {
            values,
            frequencies,
            labels
        };
    }
    /**
   * Gets the current frequency domain data from the playing audio track
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {AudioAnalysisOutputType}
   */ getFrequencies() {
        let analysisType = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : 'frequency', minDecibels = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : -100, maxDecibels = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -30;
        let fftResult = null;
        if (this.audioBuffer && this.fftResults.length) {
            const pct = this.audio.currentTime / this.audio.duration;
            const index = Math.min(pct * this.fftResults.length | 0, this.fftResults.length - 1);
            fftResult = this.fftResults[index];
        }
        return AudioAnalysis.getFrequencies(this.analyser, this.sampleRate, fftResult, analysisType, minDecibels, maxDecibels);
    }
    /**
   * Resume the internal AudioContext if it was suspended due to the lack of
   * user interaction when the AudioAnalysis was instantiated.
   * @returns {Promise<true>}
   */ async resumeIfSuspended() {
        if ('suspended' === this.context.state) await this.context.resume();
        return true;
    }
    /**
   * Creates a new AudioAnalysis instance for an HTMLAudioElement
   * @param {HTMLAudioElement} audioElement
   * @param {AudioBuffer|null} [audioBuffer] If provided, will cache all frequency domain data from the buffer
   * @returns {AudioAnalysis}
   */ constructor(audioElement, audioBuffer = null){
        this.fftResults = [];
        if (audioBuffer) {
            /**
       * Modified from
       * https://stackoverflow.com/questions/75063715/using-the-web-audio-api-to-analyze-a-song-without-playing
       *
       * We do this to populate FFT values for the audio if provided an `audioBuffer`
       * The reason to do this is that Safari fails when using `createMediaElementSource`
       * This has a non-zero RAM cost so we only opt-in to run it on Safari, Chrome is better
       */ const { length, sampleRate } = audioBuffer;
            const offlineAudioContext = new OfflineAudioContext({
                length,
                sampleRate
            });
            const source = offlineAudioContext.createBufferSource();
            source.buffer = audioBuffer;
            const analyser = offlineAudioContext.createAnalyser();
            analyser.fftSize = 8192;
            analyser.smoothingTimeConstant = 0.1;
            source.connect(analyser);
            // limit is :: 128 / sampleRate;
            // but we just want 60fps - cuts ~1s from 6MB to 1MB of RAM
            const renderQuantumInSeconds = 1 / 60;
            const durationInSeconds = length / sampleRate;
            const analyze = (index)=>{
                const suspendTime = renderQuantumInSeconds * index;
                if (suspendTime < durationInSeconds) offlineAudioContext.suspend(suspendTime).then(()=>{
                    const fftResult = new Float32Array(analyser.frequencyBinCount);
                    analyser.getFloatFrequencyData(fftResult);
                    this.fftResults.push(fftResult);
                    analyze(index + 1);
                });
                if (1 === index) offlineAudioContext.startRendering();
                else offlineAudioContext.resume();
            };
            source.start(0);
            analyze(1);
            this.audio = audioElement;
            this.context = offlineAudioContext;
            this.analyser = analyser;
            this.sampleRate = sampleRate;
            this.audioBuffer = audioBuffer;
        } else {
            const audioContext = new AudioContext();
            const track = audioContext.createMediaElementSource(audioElement);
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 8192;
            analyser.smoothingTimeConstant = 0.1;
            track.connect(analyser);
            analyser.connect(audioContext.destination);
            this.audio = audioElement;
            this.context = audioContext;
            this.analyser = analyser;
            this.sampleRate = this.context.sampleRate;
            this.audioBuffer = null;
        }
    }
}
globalThis.AudioAnalysis = AudioAnalysis;
/**
 * Plays audio streams received in raw PCM16 chunks from the browser
 * @class
 */ class WavStreamPlayer {
    /**
   * Connects the audio context and enables output to speakers
   * @returns {Promise<true>}
   */ async connect() {
        this.context = new AudioContext({
            sampleRate: this.sampleRate
        });
        if ('suspended' === this.context.state) await this.context.resume();
        try {
            await this.context.audioWorklet.addModule(this.scriptSrc);
        } catch (e) {
            console.error(e);
            throw new Error(`Could not add audioWorklet module: ${this.scriptSrc}`);
        }
        const analyser = this.context.createAnalyser();
        analyser.fftSize = 8192;
        analyser.smoothingTimeConstant = 0.1;
        this.analyser = analyser;
        return true;
    }
    async pause() {
        if (this.context && !this.isPaused) {
            await this.context.suspend();
            this.isPaused = true;
        }
    }
    async resume() {
        if (this.context && this.isPaused) {
            await this.context.resume();
            this.isPaused = false;
        }
    }
    async togglePlay() {
        if (this.isPaused) await this.resume();
        else await this.pause();
    }
    isPlaying() {
        return this.context && this.stream && !this.isPaused && 'running' === this.context.state;
    }
    /**
   * Gets the current frequency domain data from the playing track
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {import('./analysis/audio_analysis.js').AudioAnalysisOutputType}
   */ getFrequencies() {
        let analysisType = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : 'frequency', minDecibels = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : -100, maxDecibels = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -30;
        if (!this.analyser) throw new Error('Not connected, please call .connect() first');
        return AudioAnalysis.getFrequencies(this.analyser, this.sampleRate, null, analysisType, minDecibels, maxDecibels);
    }
    /**
   * Starts audio streaming
   * @private
   * @returns {Promise<true>}
   */ async _start() {
        // Ensure worklet is loaded
        if (!this.context) await this.connect();
        const streamNode = new AudioWorkletNode(this.context, 'stream_processor');
        streamNode.connect(this.context.destination);
        streamNode.port.onmessage = (e)=>{
            const { event } = e.data;
            if ('stop' === event) {
                streamNode.disconnect();
                this.stream = null;
            } else if ('offset' === event) {
                const { requestId, trackId, offset } = e.data;
                const currentTime = offset / this.sampleRate;
                this.trackSampleOffsets[requestId] = {
                    trackId,
                    offset,
                    currentTime
                };
            }
        };
        this.analyser.disconnect();
        streamNode.connect(this.analyser);
        this.stream = streamNode;
        return true;
    }
    /**
   * Adds 16BitPCM data to the currently playing audio stream
   * You can add chunks beyond the current play point and they will be queued for play
   * @param {ArrayBuffer|Int16Array} arrayBuffer
   * @param {string} [trackId]
   * @returns {Int16Array}
   */ async add16BitPCM(arrayBuffer) {
        let trackId = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 'default';
        if ('string' != typeof trackId) throw new Error("trackId must be a string");
        if (this.interruptedTrackIds[trackId]) return;
        if (!this.stream) await this._start();
        let buffer;
        if (arrayBuffer instanceof Int16Array) buffer = arrayBuffer;
        else if (arrayBuffer instanceof ArrayBuffer) buffer = new Int16Array(arrayBuffer);
        else throw new Error("argument must be Int16Array or ArrayBuffer");
        this.stream.port.postMessage({
            event: 'write',
            buffer,
            trackId
        });
        return buffer;
    }
    /**
   * Gets the offset (sample count) of the currently playing stream
   * @param {boolean} [interrupt]
   * @returns {{trackId: string|null, offset: number, currentTime: number}}
   */ async getTrackSampleOffset() {
        let interrupt = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        if (!this.stream) return null;
        const requestId = crypto.randomUUID();
        this.stream.port.postMessage({
            event: interrupt ? 'interrupt' : 'offset',
            requestId
        });
        let trackSampleOffset;
        while(!trackSampleOffset){
            trackSampleOffset = this.trackSampleOffsets[requestId];
            await new Promise((r)=>setTimeout(()=>r(), 1));
        }
        const { trackId } = trackSampleOffset;
        if (interrupt && trackId) this.interruptedTrackIds[trackId] = true;
        return trackSampleOffset;
    }
    /**
   * Strips the current stream and returns the sample offset of the audio
   * @param {boolean} [interrupt]
   * @returns {{trackId: string|null, offset: number, currentTime: number}}
   */ async interrupt() {
        return this.getTrackSampleOffset(true);
    }
    /**
   * Creates a new WavStreamPlayer instance
   * @param {{sampleRate?: number}} options
   * @returns {WavStreamPlayer}
   */ constructor({ sampleRate = 44100 } = {}){
        this.scriptSrc = StreamProcessorSrc;
        this.sampleRate = sampleRate;
        this.context = null;
        this.stream = null;
        this.analyser = null;
        this.trackSampleOffsets = {};
        this.interruptedTrackIds = {};
        this.isPaused = false;
    }
}
globalThis.WavStreamPlayer = WavStreamPlayer;
const AudioProcessorWorklet = `
class AudioProcessor extends AudioWorkletProcessor {

  constructor() {
    super();
    this.port.onmessage = this.receive.bind(this);
    this.initialize();
  }

  initialize() {
    this.foundAudio = false;
    this.recording = false;
    this.chunks = [];
  }

  /**
   * Concatenates sampled chunks into channels
   * Format is chunk[Left[], Right[]]
   */
  readChannelData(chunks, channel = -1, maxChannels = 9) {
    let channelLimit;
    if (channel !== -1) {
      if (chunks[0] && chunks[0].length - 1 < channel) {
        throw new Error(
          \`Channel \${channel} out of range: max \${chunks[0].length}\`
        );
      }
      channelLimit = channel + 1;
    } else {
      channel = 0;
      channelLimit = Math.min(chunks[0] ? chunks[0].length : 1, maxChannels);
    }
    const channels = [];
    for (let n = channel; n < channelLimit; n++) {
      const length = chunks.reduce((sum, chunk) => {
        return sum + chunk[n].length;
      }, 0);
      const buffers = chunks.map((chunk) => chunk[n]);
      const result = new Float32Array(length);
      let offset = 0;
      for (let i = 0; i < buffers.length; i++) {
        result.set(buffers[i], offset);
        offset += buffers[i].length;
      }
      channels[n] = result;
    }
    return channels;
  }

  /**
   * Combines parallel audio data into correct format,
   * channels[Left[], Right[]] to float32Array[LRLRLRLR...]
   */
  formatAudioData(channels) {
    if (channels.length === 1) {
      // Simple case is only one channel
      const float32Array = channels[0].slice();
      const meanValues = channels[0].slice();
      return { float32Array, meanValues };
    } else {
      const float32Array = new Float32Array(
        channels[0].length * channels.length
      );
      const meanValues = new Float32Array(channels[0].length);
      for (let i = 0; i < channels[0].length; i++) {
        const offset = i * channels.length;
        let meanValue = 0;
        for (let n = 0; n < channels.length; n++) {
          float32Array[offset + n] = channels[n][i];
          meanValue += channels[n][i];
        }
        meanValues[i] = meanValue / channels.length;
      }
      return { float32Array, meanValues };
    }
  }

  /**
   * Converts 32-bit float data to 16-bit integers
   */
  floatTo16BitPCM(float32Array) {
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);
    let offset = 0;
    for (let i = 0; i < float32Array.length; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
    return buffer;
  }

  /**
   * Retrieves the most recent amplitude values from the audio stream
   * @param {number} channel
   */
  getValues(channel = -1) {
    const channels = this.readChannelData(this.chunks, channel);
    const { meanValues } = this.formatAudioData(channels);
    return { meanValues, channels };
  }

  /**
   * Exports chunks as an audio/wav file
   */
  export() {
    const channels = this.readChannelData(this.chunks);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const audioData = this.floatTo16BitPCM(float32Array);
    return {
      meanValues: meanValues,
      audio: {
        bitsPerSample: 16,
        channels: channels,
        data: audioData,
      },
    };
  }

  receive(e) {
    const { event, id } = e.data;
    let receiptData = {};
    switch (event) {
      case 'start':
        this.recording = true;
        break;
      case 'stop':
        this.recording = false;
        break;
      case 'clear':
        this.initialize();
        break;
      case 'export':
        receiptData = this.export();
        break;
      case 'read':
        receiptData = this.getValues();
        break;
      default:
        break;
    }
    // Always send back receipt
    this.port.postMessage({ event: 'receipt', id, data: receiptData });
  }

  sendChunk(chunk) {
    const channels = this.readChannelData([chunk]);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const rawAudioData = this.floatTo16BitPCM(float32Array);
    const monoAudioData = this.floatTo16BitPCM(meanValues);
    this.port.postMessage({
      event: 'chunk',
      data: {
        mono: monoAudioData,
        raw: rawAudioData,
      },
    });
  }

  process(inputList, outputList, parameters) {
    // Copy input to output (e.g. speakers)
    // Note that this creates choppy sounds with Mac products
    const sourceLimit = Math.min(inputList.length, outputList.length);
    for (let inputNum = 0; inputNum < sourceLimit; inputNum++) {
      const input = inputList[inputNum];
      const output = outputList[inputNum];
      const channelCount = Math.min(input.length, output.length);
      for (let channelNum = 0; channelNum < channelCount; channelNum++) {
        input[channelNum].forEach((sample, i) => {
          output[channelNum][i] = sample;
        });
      }
    }
    const inputs = inputList[0];
    // There's latency at the beginning of a stream before recording starts
    // Make sure we actually receive audio data before we start storing chunks
    let sliceIndex = 0;
    if (!this.foundAudio) {
      for (const channel of inputs) {
        sliceIndex = 0; // reset for each channel
        if (this.foundAudio) {
          break;
        }
        if (channel) {
          for (const value of channel) {
            if (value !== 0) {
              // find only one non-zero entry in any channel
              this.foundAudio = true;
              break;
            } else {
              sliceIndex++;
            }
          }
        }
      }
    }
    if (inputs && inputs[0] && this.foundAudio && this.recording) {
      // We need to copy the TypedArray, because the \`process\`
      // internals will reuse the same buffer to hold each input
      const chunk = inputs.map((input) => input.slice(sliceIndex));
      this.chunks.push(chunk);
      this.sendChunk(chunk);
    }
    return true;
  }
}

registerProcessor('audio_processor', AudioProcessor);
`;
const audio_processor_script = new Blob([
    AudioProcessorWorklet
], {
    type: 'application/javascript'
});
const audio_processor_src = URL.createObjectURL(audio_processor_script);
const AudioProcessorSrc = audio_processor_src;
/**
 * Raw wav audio file contents
 * @typedef {Object} WavPackerAudioType
 * @property {Blob} blob
 * @property {string} url
 * @property {number} channelCount
 * @property {number} sampleRate
 * @property {number} duration
 */ /**
 * Utility class for assembling PCM16 "audio/wav" data
 * @class
 */ class WavPacker {
    /**
   * Converts Float32Array of amplitude data to ArrayBuffer in Int16Array format
   * @param {Float32Array} float32Array
   * @returns {ArrayBuffer}
   */ static floatTo16BitPCM(float32Array) {
        const buffer = new ArrayBuffer(2 * float32Array.length);
        const view = new DataView(buffer);
        let offset = 0;
        for(let i = 0; i < float32Array.length; i++, offset += 2){
            let s = Math.max(-1, Math.min(1, float32Array[i]));
            view.setInt16(offset, s < 0 ? 0x8000 * s : 0x7fff * s, true);
        }
        return buffer;
    }
    /**
   * Concatenates two ArrayBuffers
   * @param {ArrayBuffer} leftBuffer
   * @param {ArrayBuffer} rightBuffer
   * @returns {ArrayBuffer}
   */ static mergeBuffers(leftBuffer, rightBuffer) {
        const tmpArray = new Uint8Array(leftBuffer.byteLength + rightBuffer.byteLength);
        tmpArray.set(new Uint8Array(leftBuffer), 0);
        tmpArray.set(new Uint8Array(rightBuffer), leftBuffer.byteLength);
        return tmpArray.buffer;
    }
    /**
   * Packs data into an Int16 format
   * @private
   * @param {number} size 0 = 1x Int16, 1 = 2x Int16
   * @param {number} arg value to pack
   * @returns
   */ _packData(size, arg) {
        return [
            new Uint8Array([
                arg,
                arg >> 8
            ]),
            new Uint8Array([
                arg,
                arg >> 8,
                arg >> 16,
                arg >> 24
            ])
        ][size];
    }
    /**
   * Packs audio into "audio/wav" Blob
   * @param {number} sampleRate
   * @param {{bitsPerSample: number, channels: Array<Float32Array>, data: Int16Array}} audio
   * @returns {WavPackerAudioType}
   */ pack(sampleRate, audio) {
        if (null == audio ? void 0 : audio.bitsPerSample) {
            if (null == audio ? void 0 : audio.channels) {
                if (!(null == audio ? void 0 : audio.data)) throw new Error('Missing "data"');
            } else throw new Error('Missing "channels"');
        } else throw new Error('Missing "bitsPerSample"');
        const { bitsPerSample, channels, data } = audio;
        const output = [
            // Header
            'RIFF',
            this._packData(1, 52),
            'WAVE',
            // chunk 1
            'fmt ',
            this._packData(1, 16),
            this._packData(0, 1),
            this._packData(0, channels.length),
            this._packData(1, sampleRate),
            this._packData(1, sampleRate * channels.length * bitsPerSample / 8),
            this._packData(0, channels.length * bitsPerSample / 8),
            this._packData(0, bitsPerSample),
            // chunk 2
            'data',
            this._packData(1, channels[0].length * channels.length * bitsPerSample / 8),
            data
        ];
        const blob = new Blob(output, {
            type: 'audio/mpeg'
        });
        const url = URL.createObjectURL(blob);
        return {
            blob,
            url,
            channelCount: channels.length,
            sampleRate,
            duration: data.byteLength / (channels.length * sampleRate * 2)
        };
    }
}
globalThis.WavPacker = WavPacker;
/**
 * Decodes audio into a wav file
 * @typedef {Object} DecodedAudioType
 * @property {Blob} blob
 * @property {string} url
 * @property {Float32Array} values
 * @property {AudioBuffer} audioBuffer
 */ /**
 * Records live stream of user audio as PCM16 "audio/wav" data
 * @class
 */ class WavRecorder {
    /**
   * Decodes audio data from multiple formats to a Blob, url, Float32Array and AudioBuffer
   * @param {Blob|Float32Array|Int16Array|ArrayBuffer|number[]} audioData
   * @param {number} sampleRate
   * @param {number} fromSampleRate
   * @returns {Promise<DecodedAudioType>}
   */ static async decode(audioData) {
        let sampleRate = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 44100, fromSampleRate = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -1;
        const context = new AudioContext({
            sampleRate
        });
        let arrayBuffer;
        let blob;
        if (audioData instanceof Blob) {
            if (-1 !== fromSampleRate) throw new Error('Can not specify "fromSampleRate" when reading from Blob');
            blob = audioData;
            arrayBuffer = await blob.arrayBuffer();
        } else if (audioData instanceof ArrayBuffer) {
            if (-1 !== fromSampleRate) throw new Error('Can not specify "fromSampleRate" when reading from ArrayBuffer');
            arrayBuffer = audioData;
            blob = new Blob([
                arrayBuffer
            ], {
                type: 'audio/wav'
            });
        } else {
            let float32Array;
            let data;
            if (audioData instanceof Int16Array) {
                data = audioData;
                float32Array = new Float32Array(audioData.length);
                for(let i = 0; i < audioData.length; i++)float32Array[i] = audioData[i] / 0x8000;
            } else if (audioData instanceof Float32Array) float32Array = audioData;
            else if (audioData instanceof Array) float32Array = new Float32Array(audioData);
            else throw new Error('"audioData" must be one of: Blob, Float32Arrray, Int16Array, ArrayBuffer, Array<number>');
            if (-1 === fromSampleRate) throw new Error('Must specify "fromSampleRate" when reading from Float32Array, In16Array or Array');
            if (fromSampleRate < 3000) throw new Error('Minimum "fromSampleRate" is 3000 (3kHz)');
            if (!data) data = WavPacker.floatTo16BitPCM(float32Array);
            const audio = {
                bitsPerSample: 16,
                channels: [
                    float32Array
                ],
                data
            };
            const packer = new WavPacker();
            const result = packer.pack(fromSampleRate, audio);
            blob = result.blob;
            arrayBuffer = await blob.arrayBuffer();
        }
        const audioBuffer = await context.decodeAudioData(arrayBuffer);
        const values = audioBuffer.getChannelData(0);
        const url = URL.createObjectURL(blob);
        return {
            blob,
            url,
            values,
            audioBuffer
        };
    }
    /**
   * Logs data in debug mode
   * @param {...any} arguments
   * @returns {true}
   */ log() {
        if (this.debug) this.log(...arguments);
        return true;
    }
    /**
   * Retrieves the current sampleRate for the recorder
   * @returns {number}
   */ getSampleRate() {
        return this.sampleRate;
    }
    /**
   * Retrieves the current status of the recording
   * @returns {"ended"|"paused"|"recording"}
   */ getStatus() {
        if (!this.processor) return 'ended';
        if (!this.recording) return 'paused';
        return 'recording';
    }
    /**
   * Sends an event to the AudioWorklet
   * @private
   * @param {string} name
   * @param {{[key: string]: any}} data
   * @param {AudioWorkletNode} [_processor]
   * @returns {Promise<{[key: string]: any}>}
   */ async _event(name) {
        let data = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {}, _processor = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : null;
        _processor = _processor || this.processor;
        if (!_processor) throw new Error('Can not send events without recording first');
        const message = {
            event: name,
            id: this._lastEventId++,
            data
        };
        _processor.port.postMessage(message);
        const t0 = new Date().valueOf();
        while(!this.eventReceipts[message.id]){
            if (new Date().valueOf() - t0 > this.eventTimeout) throw new Error(`Timeout waiting for "${name}" event`);
            await new Promise((res)=>setTimeout(()=>res(true), 1));
        }
        const payload = this.eventReceipts[message.id];
        delete this.eventReceipts[message.id];
        return payload;
    }
    /**
   * Sets device change callback, remove if callback provided is `null`
   * @param {(Array<MediaDeviceInfo & {default: boolean}>): void|null} callback
   * @returns {true}
   */ listenForDeviceChange(callback) {
        if (null === callback && this._deviceChangeCallback) {
            navigator.mediaDevices.removeEventListener('devicechange', this._deviceChangeCallback);
            this._deviceChangeCallback = null;
        } else if (null !== callback) {
            // Basically a debounce; we only want this called once when devices change
            // And we only want the most recent callback() to be executed
            // if a few are operating at the same time
            let lastId = 0;
            let lastDevices = [];
            const serializeDevices = (devices)=>devices.map((d)=>d.deviceId).sort().join(',');
            const cb = async ()=>{
                let id = ++lastId;
                const devices = await this.listDevices();
                if (id === lastId) {
                    if (serializeDevices(lastDevices) !== serializeDevices(devices)) {
                        lastDevices = devices;
                        callback(devices.slice());
                    }
                }
            };
            navigator.mediaDevices.addEventListener('devicechange', cb);
            cb();
            this._deviceChangeCallback = cb;
        }
        return true;
    }
    /**
   * Manually request permission to use the microphone
   * @returns {Promise<true>}
   */ async requestPermission() {
        const permissionStatus = await navigator.permissions.query({
            name: 'microphone'
        });
        if ('denied' === permissionStatus.state) window.alert('You must grant microphone access to use this feature.');
        else if ('prompt' === permissionStatus.state) try {
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: true
            });
            const tracks = stream.getTracks();
            tracks.forEach((track)=>track.stop());
        } catch (e) {
            window.alert('You must grant microphone access to use this feature.');
        }
        return true;
    }
    /**
   * List all eligible devices for recording, will request permission to use microphone
   * @returns {Promise<Array<MediaDeviceInfo & {default: boolean}>>}
   */ async listDevices() {
        if (!navigator.mediaDevices || !('enumerateDevices' in navigator.mediaDevices)) throw new Error('Could not request user devices');
        await this.requestPermission();
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioDevices = devices.filter((device)=>'audioinput' === device.kind);
        const defaultDeviceIndex = audioDevices.findIndex((device)=>'default' === device.deviceId);
        const deviceList = [];
        if (-1 !== defaultDeviceIndex) {
            let defaultDevice = audioDevices.splice(defaultDeviceIndex, 1)[0];
            let existingIndex = audioDevices.findIndex((device)=>device.groupId === defaultDevice.groupId);
            if (-1 !== existingIndex) defaultDevice = audioDevices.splice(existingIndex, 1)[0];
            defaultDevice.default = true;
            deviceList.push(defaultDevice);
        }
        return deviceList.concat(audioDevices);
    }
    /**
   * Begins a recording session and requests microphone permissions if not already granted
   * Microphone recording indicator will appear on browser tab but status will be "paused"
   * @param {string} [deviceId] if no device provided, default device will be used
   * @returns {Promise<true>}
   */ async begin(deviceId) {
        if (this.processor) throw new Error("Already connected: please call .end() to start a new session");
        if (!navigator.mediaDevices || !('getUserMedia' in navigator.mediaDevices)) throw new Error('Could not request user media');
        try {
            const config = {
                audio: true
            };
            if (deviceId) config.audio = {
                deviceId: {
                    exact: deviceId
                }
            };
            this.stream = await navigator.mediaDevices.getUserMedia(config);
        } catch (err) {
            throw new Error('Could not start media stream');
        }
        const context = new AudioContext({
            sampleRate: this.sampleRate
        });
        const source = context.createMediaStreamSource(this.stream);
        // Load and execute the module script.
        try {
            await context.audioWorklet.addModule(this.scriptSrc);
        } catch (e) {
            console.error(e);
            throw new Error(`Could not add audioWorklet module: ${this.scriptSrc}`);
        }
        const processor = new AudioWorkletNode(context, 'audio_processor');
        processor.port.onmessage = (e)=>{
            const { event, id, data } = e.data;
            if ('receipt' === event) this.eventReceipts[id] = data;
            else if ('chunk' === event) {
                if (this._chunkProcessorSize) {
                    const buffer = this._chunkProcessorBuffer;
                    this._chunkProcessorBuffer = {
                        raw: WavPacker.mergeBuffers(buffer.raw, data.raw),
                        mono: WavPacker.mergeBuffers(buffer.mono, data.mono)
                    };
                    if (this._chunkProcessorBuffer.mono.byteLength >= this._chunkProcessorSize) {
                        this._chunkProcessor(this._chunkProcessorBuffer);
                        this._chunkProcessorBuffer = {
                            raw: new ArrayBuffer(0),
                            mono: new ArrayBuffer(0)
                        };
                    }
                } else this._chunkProcessor(data);
            }
        };
        const node = source.connect(processor);
        const analyser = context.createAnalyser();
        analyser.fftSize = 8192;
        analyser.smoothingTimeConstant = 0.1;
        node.connect(analyser);
        if (this.outputToSpeakers) {
            // eslint-disable-next-line no-console
            console.warn("Warning: Output to speakers may affect sound quality,\nespecially due to system audio feedback preventative measures.\nuse only for debugging");
            analyser.connect(context.destination);
        }
        this.source = source;
        this.node = node;
        this.analyser = analyser;
        this.processor = processor;
        return true;
    }
    /**
   * Gets the current frequency domain data from the recording track
   * @param {"frequency"|"music"|"voice"} [analysisType]
   * @param {number} [minDecibels] default -100
   * @param {number} [maxDecibels] default -30
   * @returns {import('./analysis/audio_analysis.js').AudioAnalysisOutputType}
   */ getFrequencies() {
        let analysisType = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : 'frequency', minDecibels = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : -100, maxDecibels = arguments.length > 2 && void 0 !== arguments[2] ? arguments[2] : -30;
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        return AudioAnalysis.getFrequencies(this.analyser, this.sampleRate, null, analysisType, minDecibels, maxDecibels);
    }
    /**
   * Pauses the recording
   * Keeps microphone stream open but halts storage of audio
   * @returns {Promise<true>}
   */ async pause() {
        if (this.processor) {
            if (!this.recording) throw new Error('Already paused: please call .record() first');
        } else throw new Error('Session ended: please call .begin() first');
        if (this._chunkProcessorBuffer.raw.byteLength) this._chunkProcessor(this._chunkProcessorBuffer);
        this.log('Pausing ...');
        await this._event('stop');
        this.recording = false;
        return true;
    }
    /**
   * Start recording stream and storing to memory from the connected audio source
   * @param {(data: { mono: Int16Array; raw: Int16Array }) => any} [chunkProcessor]
   * @param {number} [chunkSize] chunkProcessor will not be triggered until this size threshold met in mono audio
   * @returns {Promise<true>}
   */ async record() {
        let chunkProcessor = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : ()=>{}, chunkSize = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : 8192;
        if (this.processor) {
            if (this.recording) throw new Error('Already recording: please call .pause() first');
            else if ('function' != typeof chunkProcessor) throw new Error("chunkProcessor must be a function");
        } else throw new Error('Session ended: please call .begin() first');
        this._chunkProcessor = chunkProcessor;
        this._chunkProcessorSize = chunkSize;
        this._chunkProcessorBuffer = {
            raw: new ArrayBuffer(0),
            mono: new ArrayBuffer(0)
        };
        this.log('Recording ...');
        await this._event('start');
        this.recording = true;
        return true;
    }
    /**
   * Clears the audio buffer, empties stored recording
   * @returns {Promise<true>}
   */ async clear() {
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        await this._event('clear');
        return true;
    }
    /**
   * Reads the current audio stream data
   * @returns {Promise<{meanValues: Float32Array, channels: Array<Float32Array>}>}
   */ async read() {
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        this.log('Reading ...');
        const result = await this._event('read');
        return result;
    }
    /**
   * Saves the current audio stream to a file
   * @param {boolean} [force] Force saving while still recording
   * @returns {Promise<import('./wav_packer.js').WavPackerAudioType>}
   */ async save() {
        let force = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        if (!force && this.recording) throw new Error('Currently recording: please call .pause() first, or call .save(true) to force');
        this.log('Exporting ...');
        const exportData = await this._event('export');
        const packer = new WavPacker();
        const result = packer.pack(this.sampleRate, exportData.audio);
        return result;
    }
    /**
   * Ends the current recording session and saves the result
   * @returns {Promise<import('./wav_packer.js').WavPackerAudioType>}
   */ async end() {
        if (!this.processor) throw new Error('Session ended: please call .begin() first');
        const _processor = this.processor;
        this.log('Stopping ...');
        await this._event('stop');
        this.recording = false;
        const tracks = this.stream.getTracks();
        tracks.forEach((track)=>track.stop());
        this.log('Exporting ...');
        const exportData = await this._event('export', {}, _processor);
        this.processor.disconnect();
        this.source.disconnect();
        this.node.disconnect();
        this.analyser.disconnect();
        this.stream = null;
        this.processor = null;
        this.source = null;
        this.node = null;
        const packer = new WavPacker();
        const result = packer.pack(this.sampleRate, exportData.audio);
        return result;
    }
    /**
   * Performs a full cleanup of WavRecorder instance
   * Stops actively listening via microphone and removes existing listeners
   * @returns {Promise<true>}
   */ async quit() {
        this.listenForDeviceChange(null);
        if (this.processor) await this.end();
        return true;
    }
    /**
   * Create a new WavRecorder instance
   * @param {{sampleRate?: number, outputToSpeakers?: boolean, debug?: boolean}} [options]
   * @returns {WavRecorder}
   */ constructor({ sampleRate = 44100, outputToSpeakers = false, debug = false } = {}){
        // Script source
        this.scriptSrc = AudioProcessorSrc;
        // Config
        this.sampleRate = sampleRate;
        this.outputToSpeakers = outputToSpeakers;
        this.debug = !!debug;
        this._deviceChangeCallback = null;
        this._devices = [];
        // State variables
        this.stream = null;
        this.processor = null;
        this.source = null;
        this.node = null;
        this.recording = false;
        // Event handling with AudioWorklet
        this._lastEventId = 0;
        this.eventReceipts = {};
        this.eventTimeout = 5000;
        // Process chunks of audio
        this._chunkProcessor = ()=>{};
        this._chunkProcessorSize = void 0;
        this._chunkProcessorBuffer = {
            raw: new ArrayBuffer(0),
            mono: new ArrayBuffer(0)
        };
    }
}
globalThis.WavRecorder = WavRecorder;
class APIResource {
    constructor(client){
        this._client = client;
    }
}
/* eslint-disable @typescript-eslint/no-namespace */ class Bots extends APIResource {
    /**
   * Create a new agent. | 调用接口创建一个新的智能体。
   * @docs en:https://www.coze.com/docs/developer_guides/create_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/create_bot?_lang=zh
   * @param params - Required The parameters for creating a bot. | 创建 Bot 的参数。
   * @param params.space_id - Required The Space ID of the space where the agent is located. | Bot 所在的空间的 Space ID。
   * @param params.name - Required The name of the agent. It should be 1 to 20 characters long. | Bot 的名称。
   * @param params.description - Optional The description of the agent. It can be 0 to 500 characters long. | Bot 的描述信息。
   * @param params.icon_file_id - Optional The file ID for the agent's avatar. | 作为智能体头像的文件 ID。
   * @param params.prompt_info - Optional The personality and reply logic of the agent. | Bot 的提示词配置。
   * @param params.onboarding_info - Optional The settings related to the agent's opening remarks. | Bot 的开场白配置。
   * @returns Information about the created bot. | 创建的 Bot 信息。
   */ async create(params, options) {
        const apiUrl = '/v1/bot/create';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Update the configuration of an agent. | 调用接口修改智能体的配置。
   * @docs en:https://www.coze.com/docs/developer_guides/update_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/update_bot?_lang=zh
   * @param params - Required The parameters for updating a bot. | 修改 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 待修改配置的智能体ID。
   * @param params.name - Optional The name of the agent. | Bot 的名称。
   * @param params.description - Optional The description of the agent. | Bot 的描述信息。
   * @param params.icon_file_id - Optional The file ID for the agent's avatar. | 作为智能体头像的文件 ID。
   * @param params.prompt_info - Optional The personality and reply logic of the agent. | Bot 的提示词配置。
   * @param params.onboarding_info - Optional The settings related to the agent's opening remarks. | Bot 的开场白配置。
   * @param params.knowledge - Optional Knowledge configurations of the agent. | Bot 的知识库配置。
   * @returns Undefined | 无返回值
   */ async update(params, options) {
        const apiUrl = '/v1/bot/update';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Get the agents published as API service. | 调用接口查看指定空间发布到 Agent as API 渠道的智能体列表。
   * @docs en:https://www.coze.com/docs/developer_guides/published_bots_list?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/published_bots_list?_lang=zh
   * @param params - Required The parameters for listing bots. | 列出 Bot 的参数。
   * @param params.space_id - Required The ID of the space. | Bot 所在的空间的 Space ID。
   * @param params.page_size - Optional Pagination size. | 分页大小。
   * @param params.page_index - Optional Page number for paginated queries. | 分页查询时的页码。
   * @returns List of published bots. | 已发布的 Bot 列表。
   */ async list(params, options) {
        const apiUrl = '/v1/space/published_bots_list';
        const result = await this._client.get(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Publish the specified agent as an API service. | 调用接口创建一个新的智能体。
   * @docs en:https://www.coze.com/docs/developer_guides/publish_bot?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/publish_bot?_lang=zh
   * @param params - Required The parameters for publishing a bot. | 发布 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 要发布的智能体ID。
   * @param params.connector_ids - Required The list of publishing channel IDs for the agent. | 智能体的发布渠道 ID 列表。
   * @returns Undefined | 无返回值
   */ async publish(params, options) {
        const apiUrl = '/v1/bot/publish';
        const result = await this._client.post(apiUrl, params, false, options);
        return result.data;
    }
    /**
   * Get the configuration information of the agent. | 获取指定智能体的配置信息。
   * @docs en:https://www.coze.com/docs/developer_guides/get_metadata?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/get_metadata?_lang=zh
   * @param params - Required The parameters for retrieving a bot. | 获取 Bot 的参数。
   * @param params.bot_id - Required The ID of the agent that the API interacts with. | 要查看的智能体ID。
   * @returns Information about the bot. | Bot 的配置信息。
   */ async retrieve(params, options) {
        const apiUrl = '/v1/bot/get_online_info';
        const result = await this._client.get(apiUrl, params, false, options);
        return result.data;
    }
}
/* eslint-disable security/detect-object-injection */ /* eslint-disable @typescript-eslint/no-explicit-any */ function safeJsonParse(jsonString) {
    let defaultValue = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : '';
    try {
        return JSON.parse(jsonString);
    } catch (error) {
        return defaultValue;
    }
}
function utils_sleep(ms) {
    return new Promise((resolve)=>{
        setTimeout(resolve, ms);
    });
}
function utils_isBrowser() {
    return 'undefined' != typeof window;
}
function isPlainObject(obj) {
    if ('object' != typeof obj || null === obj) return false;
    const proto = Object.getPrototypeOf(obj);
    if (null === proto) return true;
    let baseProto = proto;
    while(null !== Object.getPrototypeOf(baseProto))baseProto = Object.getPrototypeOf(baseProto);
    return proto === baseProto;
}
function mergeConfig() {
    for(var _len = arguments.length, objects = new Array(_len), _key = 0; _key < _len; _key++)objects[_key] = arguments[_key];
    return objects.reduce((result, obj)=>{
        if (void 0 === obj) return result || {};
        for(const key in obj)if (Object.prototype.hasOwnProperty.call(obj, key)) {
            if (isPlainObject(obj[key]) && !Array.isArray(obj[key])) result[key] = mergeConfig(result[key] || {}, obj[key]);
            else result[key] = obj[key];
        }
        return result;
    }, {});
}
function isPersonalAccessToken(token) {
    return null == token ? void 0 : token.startsWith('pat_');
}
/* eslint-disable max-params */ class CozeError extends Error {
}
class error_APIError extends CozeError {
    static makeMessage(status, errorBody, message, headers) {
        if (!errorBody && message) return message;
        if (errorBody) {
            const list = [];
            const { code, msg, error } = errorBody;
            if (code) list.push(`code: ${code}`);
            if (msg) list.push(`msg: ${msg}`);
            if ((null == error ? void 0 : error.detail) && msg !== error.detail) list.push(`detail: ${error.detail}`);
            const logId = (null == error ? void 0 : error.logid) || (null == headers ? void 0 : headers['x-tt-logid']);
            if (logId) list.push(`logid: ${logId}`);
            return list.join(', ');
        }
        if (status) return `http status code: ${status} (no body)`;
        return '(no status code or body)';
    }
    static generate(status, errorResponse, message, headers) {
        if (!status) return new APIConnectionError({
            cause: castToError(errorResponse)
        });
        const error = errorResponse;
        // https://www.coze.cn/docs/developer_guides/coze_error_codes
        if (400 === status || (null == error ? void 0 : error.code) === 4000) return new BadRequestError(status, error, message, headers);
        if (401 === status || (null == error ? void 0 : error.code) === 4100) return new AuthenticationError(status, error, message, headers);
        if (403 === status || (null == error ? void 0 : error.code) === 4101) return new PermissionDeniedError(status, error, message, headers);
        if (404 === status || (null == error ? void 0 : error.code) === 4200) return new NotFoundError(status, error, message, headers);
        if (429 === status || (null == error ? void 0 : error.code) === 4013) return new RateLimitError(status, error, message, headers);
        if (408 === status) return new TimeoutError(status, error, message, headers);
        if (502 === status) return new GatewayError(status, error, message, headers);
        if (status >= 500) return new InternalServerError(status, error, message, headers);
        return new error_APIError(status, error, message, headers);
    }
    constructor(status, error, message, headers){
        var _error_detail, _error_error;
        super(`${error_APIError.makeMessage(status, error, message, headers)}`);
        this.status = status;
        this.headers = headers;
        this.logid = (null == error ? void 0 : null === (_error_detail = error.detail) || void 0 === _error_detail ? void 0 : _error_detail.logid) || (null == headers ? void 0 : headers['x-tt-logid']);
        // this.error = error;
        this.code = null == error ? void 0 : error.code;
        this.msg = null == error ? void 0 : error.msg;
        this.detail = null == error ? void 0 : null === (_error_error = error.error) || void 0 === _error_error ? void 0 : _error_error.detail;
        this.rawError = error;
    }
}
class APIConnectionError extends error_APIError {
    constructor({ message }){
        super(void 0, void 0, message || 'Connection error.', void 0), this.status = void 0;
    // if (cause) {
    //   this.cause = cause;
    // }
    }
}
class APIUserAbortError extends error_APIError {
    constructor(message){
        super(void 0, void 0, message || 'Request was aborted.', void 0), this.name = 'UserAbortError', this.status = void 0;
    }
}
class BadRequestError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'BadRequestError', this.status = 400;
    }
}
class AuthenticationError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'AuthenticationError', this.status = 401;
    }
}
class PermissionDeniedError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'PermissionDeniedError', this.status = 403;
    }
}
class NotFoundError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'NotFoundError', this.status = 404;
    }
}
class TimeoutError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'TimeoutError', this.status = 408;
    }
}
class RateLimitError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'RateLimitError', this.status = 429;
    }
}
class InternalServerError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'InternalServerError', this.status = 500;
    }
}
class GatewayError extends error_APIError {
    constructor(...args){
        super(...args), this.name = 'GatewayError', this.status = 502;
    }
}
const castToError = (err)=>{
    if (err instanceof Error) return err;
    return new Error(err);
};
class Messages extends APIResource {
    /**
   * Get the list of messages in a chat. | 获取对话中的消息列表。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_message_list?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_message_list?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns An array of chat messages. | 对话消息数组。
   */ async list(conversation_id, chat_id, options) {
        const apiUrl = `/v3/chat/message/list?conversation_id=${conversation_id}&chat_id=${chat_id}`;
        const result = await this._client.get(apiUrl, void 0, false, options);
        return result.data;
    }
}
const uuid = ()=>(Math.random() * new Date().getTime()).toString();
const handleAdditionalMessages = (additional_messages)=>null == additional_messages ? void 0 : additional_messages.map((i)=>({
            ...i,
            content: 'object' == typeof i.content ? JSON.stringify(i.content) : i.content
        }));
const handleParameters = (parameters)=>{
    if (parameters) {
        for (const [key, value] of Object.entries(parameters))if ('object' == typeof value) parameters[key] = JSON.stringify(value);
    }
    return parameters;
};
class Chat extends APIResource {
    /**
   * Call the Chat API to send messages to a published Coze agent. | 调用此接口发起一次对话，支持添加上下文
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for creating a chat session. | 创建会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @returns The data of the created chat. | 创建的对话数据。
   */ async create(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: false
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        return result.data;
    }
    /**
   * Call the Chat API to send messages to a published Coze agent. | 调用此接口发起一次对话，支持添加上下文
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for creating a chat session. | 创建会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义的变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @returns
   */ async createAndPoll(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: false
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        const chatId = result.data.id;
        const conversationId = result.data.conversation_id;
        let chat;
        while(true){
            await utils_sleep(100);
            chat = await this.retrieve(conversationId, chatId);
            if ('completed' === chat.status || 'failed' === chat.status || 'requires_action' === chat.status) break;
        }
        const messageList = await this.messages.list(conversationId, chatId);
        return {
            chat,
            messages: messageList
        };
    }
    /**
   * Call the Chat API to send messages to a published Coze agent with streaming response. | 调用此接口发起一次对话，支持流式响应。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_v3?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_v3?_lang=zh
   * @param params - Required The parameters for streaming a chat session. | 流式会话的参数。
   * @param params.bot_id - Required The ID of the agent. | 要进行会话聊天的 Bot ID。
   * @param params.user_id - Optional The ID of the user interacting with the Bot. | 标识当前与 Bot 交互的用户。
   * @param params.additional_messages - Optional Additional messages for the conversation. | 对话的附加信息。
   * @param params.custom_variables - Optional Variables defined in the Bot. | Bot 中定义的变量。
   * @param params.auto_save_history - Optional Whether to automatically save the conversation history. | 是否自动保存历史对话记录。
   * @param params.meta_data - Optional Additional metadata for the message. | 创建消息时的附加消息。
   * @param params.conversation_id - Optional The ID of the conversation. | 标识对话发生在哪一次会话中。
   * @param params.extra_params - Optional Extra parameters for the conversation. | 附加参数。
   * @returns A stream of chat data. | 对话数据流。
   */ async *stream(params, options) {
        if (!params.user_id) params.user_id = uuid();
        const { conversation_id, ...rest } = params;
        const apiUrl = `/v3/chat${conversation_id ? `?conversation_id=${conversation_id}` : ''}`;
        const payload = {
            ...rest,
            additional_messages: handleAdditionalMessages(params.additional_messages),
            shortcut_command: params.shortcut_command ? {
                ...params.shortcut_command,
                parameters: handleParameters(params.shortcut_command.parameters)
            } : void 0,
            stream: true
        };
        const result = await this._client.post(apiUrl, payload, true, options);
        for await (const message of result)if ("done" === message.event) {
            const ret = {
                event: message.event,
                data: '[DONE]'
            };
            yield ret;
        } else try {
            const ret = {
                event: message.event,
                data: JSON.parse(message.data)
            };
            yield ret;
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
    /**
   * Get the detailed information of the chat. | 查看对话的详细信息。
   * @docs en:https://www.coze.com/docs/developer_guides/retrieve_chat?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/retrieve_chat?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns The data of the retrieved chat. | 检索到的对话数据。
   */ async retrieve(conversation_id, chat_id, options) {
        const apiUrl = `/v3/chat/retrieve?conversation_id=${conversation_id}&chat_id=${chat_id}`;
        const result = await this._client.post(apiUrl, void 0, false, options);
        return result.data;
    }
    /**
   * Cancel a chat session. | 取消对话会话。
   * @docs en:https://www.coze.com/docs/developer_guides/cancel_chat?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/cancel_chat?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param chat_id - Required The ID of the chat. | 对话 ID。
   * @returns The data of the canceled chat. | 取消的对话数据。
   */ async cancel(conversation_id, chat_id, options) {
        const apiUrl = '/v3/chat/cancel';
        const payload = {
            conversation_id,
            chat_id
        };
        const result = await this._client.post(apiUrl, payload, false, options);
        return result.data;
    }
    /**
   * Submit tool outputs for a chat session. | 提交对话会话的工具输出。
   * @docs en:https://www.coze.com/docs/developer_guides/chat_submit_tool_outputs?_lang=en
   * @docs zh:https://www.coze.cn/docs/developer_guides/chat_submit_tool_outputs?_lang=zh
   * @param params - Required Parameters for submitting tool outputs. | 提交工具输出的参数。
   * @param params.conversation_id - Required The ID of the conversation. | 会话 ID。
   * @param params.chat_id - Required The ID of the chat. | 对话 ID。
   * @param params.tool_outputs - Required The outputs of the tool. | 工具的输出。
   * @param params.stream - Optional Whether to use streaming response. | 是否使用流式响应。
   * @returns The data of the submitted tool outputs or a stream of chat data. | 提交的工具输出数据或对话数据流。
   */ async *submitToolOutputs(params, options) {
        const { conversation_id, chat_id, ...rest } = params;
        const apiUrl = `/v3/chat/submit_tool_outputs?conversation_id=${params.conversation_id}&chat_id=${params.chat_id}`;
        const payload = {
            ...rest
        };
        if (false === params.stream) {
            const response = await this._client.post(apiUrl, payload, false, options);
            return response.data;
        }
        {
            const result = await this._client.post(apiUrl, payload, true, options);
            for await (const message of result)if ("done" === message.event) {
                const ret = {
                    event: message.event,
                    data: '[DONE]'
                };
                yield ret;
            } else try {
                const ret = {
                    event: message.event,
                    data: JSON.parse(message.data)
                };
                yield ret;
            } catch (error) {
                throw new CozeError(`Could not parse message into JSON:${message.data}`);
            }
        }
    }
    constructor(...args){
        super(...args), this.messages = new Messages(this._client);
    }
}
var chat_ChatEventType = /*#__PURE__*/ function(ChatEventType) {
    ChatEventType["CONVERSATION_CHAT_CREATED"] = "conversation.chat.created";
    ChatEventType["CONVERSATION_CHAT_IN_PROGRESS"] = "conversation.chat.in_progress";
    ChatEventType["CONVERSATION_CHAT_COMPLETED"] = "conversation.chat.completed";
    ChatEventType["CONVERSATION_CHAT_FAILED"] = "conversation.chat.failed";
    ChatEventType["CONVERSATION_CHAT_REQUIRES_ACTION"] = "conversation.chat.requires_action";
    ChatEventType["CONVERSATION_MESSAGE_DELTA"] = "conversation.message.delta";
    ChatEventType["CONVERSATION_MESSAGE_COMPLETED"] = "conversation.message.completed";
    ChatEventType["CONVERSATION_AUDIO_DELTA"] = "conversation.audio.delta";
    ChatEventType["DONE"] = "done";
    ChatEventType["ERROR"] = "error";
    return ChatEventType;
}({});
class messages_Messages extends APIResource {
    /**
   * Create a message and add it to the specified conversation. | 创建一条消息，并将其添加到指定的会话中。
   * @docs en: https://www.coze.com/docs/developer_guides/create_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param params - Required The parameters for creating a message | 创建消息所需的参数
   * @param params.role - Required The entity that sent this message. Possible values: user, assistant. | 发送这条消息的实体。取值：user, assistant。
   * @param params.content - Required The content of the message. | 消息的内容。
   * @param params.content_type - Required The type of the message content. | 消息内容的类型。
   * @param params.meta_data - Optional Additional information when creating a message. | 创建消息时的附加消息。
   * @returns Information about the new message. | 消息详情。
   */ async create(conversation_id, params, options) {
        const apiUrl = `/v1/conversation/message/create?conversation_id=${conversation_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Modify a message, supporting the modification of message content, additional content, and message type. | 修改一条消息，支持修改消息内容、附加内容和消息类型。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @param params - Required The parameters for modifying a message | 修改消息所需的参数
   * @param params.meta_data - Optional Additional information when modifying a message. | 修改消息时的附加消息。
   * @param params.content - Optional The content of the message. | 消息的内容。
   * @param params.content_type - Optional The type of the message content. | 消息内容的类型。
   * @returns Information about the modified message. | 消息详情。
   */ // eslint-disable-next-line max-params
    async update(conversation_id, message_id, params, options) {
        const apiUrl = `/v1/conversation/message/modify?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.message;
    }
    /**
   * Get the detailed information of specified message. | 查看指定消息的详细信息。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @returns Information about the message. | 消息详情。
   */ async retrieve(conversation_id, message_id, options) {
        const apiUrl = `/v1/conversation/message/retrieve?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
    /**
   * List messages in a conversation. | 列出会话中的消息。
   * @docs en: https://www.coze.com/docs/developer_guides/message_list?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/message_list?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param params - Optional The parameters for listing messages | 列出消息所需的参数
   * @param params.order - Optional The order of the messages. | 消息的顺序。
   * @param params.chat_id - Optional The ID of the chat. | 聊天 ID。
   * @param params.before_id - Optional The ID of the message before which to list. | 列出此消息之前的消息 ID。
   * @param params.after_id - Optional The ID of the message after which to list. | 列出此消息之后的消息 ID。
   * @param params.limit - Optional The maximum number of messages to return. | 返回的最大消息数。
   * @returns A list of messages. | 消息列表。
   */ async list(conversation_id, params, options) {
        const apiUrl = `/v1/conversation/message/list?conversation_id=${conversation_id}`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Call the API to delete a message within a specified conversation. | 调用接口在指定会话中删除消息。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_message?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_message?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @param message_id - Required The ID of the message. | Message ID，即消息的唯一标识。
   * @returns Details of the deleted message. | 已删除的消息详情。
   */ async delete(conversation_id, message_id, options) {
        const apiUrl = `/v1/conversation/message/delete?conversation_id=${conversation_id}&message_id=${message_id}`;
        const response = await this._client.post(apiUrl, void 0, false, options);
        return response.data;
    }
}
class Conversations extends APIResource {
    /**
   * Create a conversation. Conversation is an interaction between an agent and a user, including one or more messages. | 调用接口创建一个会话。
   * @docs en: https://www.coze.com/docs/developer_guides/create_conversation?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_conversation?_lang=zh
   * @param params - Required The parameters for creating a conversation | 创建会话所需的参数
   * @param params.messages - Optional Messages in the conversation. | 会话中的消息内容。
   * @param params.meta_data - Optional Additional information when creating a message. | 创建消息时的附加消息。
   * @param params.bot_id - Optional Bind and isolate conversation on different bots. | 绑定和隔离不同Bot的会话。
   * @returns Information about the created conversation. | 会话的基础信息。
   */ async create(params, options) {
        const apiUrl = '/v1/conversation/create';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Get the information of specific conversation. | 通过会话 ID 查看会话信息。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_conversation?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_conversation?_lang=zh
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @returns Information about the conversation. | 会话的基础信息。
   */ async retrieve(conversation_id, options) {
        const apiUrl = `/v1/conversation/retrieve?conversation_id=${conversation_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
    /**
   * List all conversations. | 列出 Bot 下所有会话。
   * @param params
   * @param params.bot_id - Required Bot ID. | Bot ID。
   * @param params.page_num - Optional The page number. | 页码，默认值为 1。
   * @param params.page_size - Optional The number of conversations per page. | 每页的会话数量，默认值为 50。
   * @returns Information about the conversations. | 会话的信息。
   */ async list(params, options) {
        const apiUrl = '/v1/conversations';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Clear a conversation. | 清空会话。
   * @param conversation_id - Required The ID of the conversation. | Conversation ID，即会话的唯一标识。
   * @returns Information about the conversation session. | 会话的会话 ID。
   */ async clear(conversation_id, options) {
        const apiUrl = `/v1/conversations/${conversation_id}/clear`;
        const response = await this._client.post(apiUrl, null, false, options);
        return response.data;
    }
    constructor(...args){
        super(...args), this.messages = new messages_Messages(this._client);
    }
}
class Files extends APIResource {
    /**
   * Upload files to Coze platform. | 调用接口上传文件到扣子。
   * @docs en: https://www.coze.com/docs/developer_guides/upload_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/upload_files?_lang=zh
   * @param params - Required The parameters for file upload | 上传文件所需的参数
   * @param params.file - Required The file to be uploaded. | 需要上传的文件。
   * @returns Information about the new file. | 已上传的文件信息。
   */ async upload(params, options) {
        const apiUrl = '/v1/files/upload';
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * Get the information of the specific file uploaded to Coze platform. | 查看已上传的文件详情。
   * @docs en: https://www.coze.com/docs/developer_guides/retrieve_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/retrieve_files?_lang=zh
   * @param file_id - Required The ID of the uploaded file. | 已上传的文件 ID。
   * @returns Information about the uploaded file. | 已上传的文件信息。
   */ async retrieve(file_id, options) {
        const apiUrl = `/v1/files/retrieve?file_id=${file_id}`;
        const response = await this._client.get(apiUrl, null, false, options);
        return response.data;
    }
}
class Runs extends APIResource {
    /**
   * Initiates a workflow run. | 启动工作流运行。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_run?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_run?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to run. | 必选 要运行的工作流 ID。
   * @param params.bot_id - Optional The ID of the bot associated with the workflow. | 可选 与工作流关联的机器人 ID。
   * @param params.parameters - Optional Parameters for the workflow execution. | 可选 工作流执行的参数。
   * @param params.ext - Optional Additional information for the workflow execution. | 可选 工作流执行的附加信息。
   * @param params.execute_mode - Optional The mode in which to execute the workflow. | 可选 工作流执行的模式。
   * @param params.connector_id - Optional The ID of the connector to use for the workflow. | 可选 用于工作流的连接器 ID。
   * @param params.app_id - Optional The ID of the app.  | 可选 要进行会话聊天的 App ID
   * @returns RunWorkflowData | 工作流运行数据
   */ async create(params, options) {
        const apiUrl = '/v1/workflow/run';
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Streams the workflow run events. | 流式传输工作流运行事件。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_stream_run?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_stream_run?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to run. | 必选 要运行的工作流 ID。
   * @param params.bot_id - Optional The ID of the bot associated with the workflow. | 可选 与工作流关联的机器人 ID。
   * @param params.parameters - Optional Parameters for the workflow execution. | 可选 工作流执行的参数。
   * @param params.ext - Optional Additional information for the workflow execution. | 可选 工作流执行的附加信息。
   * @param params.execute_mode - Optional The mode in which to execute the workflow. | 可选 工作流执行的模式。
   * @param params.connector_id - Optional The ID of the connector to use for the workflow. | 可选 用于工作流的连接器 ID。
   * @param params.app_id - Optional The ID of the app.  | 可选 要进行会话聊天的 App ID
   * @returns Stream<WorkflowEvent, { id: string; event: string; data: string }> | 工作流事件流
   */ async *stream(params, options) {
        const apiUrl = '/v1/workflow/stream_run';
        const result = await this._client.post(apiUrl, params, true, options);
        for await (const message of result)try {
            if ("Done" === message.event) yield new WorkflowEvent(Number(message.id), "Done");
            else yield new WorkflowEvent(Number(message.id), message.event, JSON.parse(message.data));
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
    /**
   * Resumes a paused workflow run. | 恢复暂停的工作流运行。
   * @docs en: https://www.coze.com/docs/developer_guides/workflow_resume?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_resume?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to resume. | 必选 要恢复的工作流 ID。
   * @param params.event_id - Required The ID of the event to resume from. | 必选 要从中恢复的事件 ID。
   * @param params.resume_data - Required Data needed to resume the workflow. | 必选 恢复工作流所需的数据。
   * @param params.interrupt_type - Required The type of interruption to resume from. | 必选 要恢复的中断类型。
   * @returns { id: string; event: WorkflowEventType; data: WorkflowEventMessage | WorkflowEventInterrupt | WorkflowEventError | null } | 恢复的工作流事件数据
   */ async resume(params, options) {
        const apiUrl = '/v1/workflow/stream_resume';
        const response = await this._client.post(apiUrl, params, false, options);
        return response;
    }
    /**
   * Get the workflow run history | 工作流异步运行后，查看执行结果
   * @docs zh: https://www.coze.cn/open/docs/developer_guides/workflow_history
   * @param workflowId - Required The ID of the workflow. | 必选 工作流 ID。
   * @param executeId - Required The ID of the workflow execution. | 必选 工作流执行 ID。
   * @returns WorkflowExecuteHistory[] | 工作流执行历史
   */ async history(workflowId, executeId, options) {
        const apiUrl = `/v1/workflows/${workflowId}/run_histories/${executeId}`;
        const response = await this._client.get(apiUrl, void 0, false, options);
        return response.data;
    }
}
class WorkflowEvent {
    constructor(id, event, data){
        this.id = id;
        this.event = event;
        this.data = data;
    }
}
class WorkflowChat extends APIResource {
    /**
   * Execute a chat workflow. | 执行对话流
   * @docs en: https://www.coze.cn/docs/developer_guides/workflow_chat?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/workflow_chat?_lang=zh
   * @param params.workflow_id - Required The ID of the workflow to chat with. | 必选 要对话的工作流 ID。
   * @param params.additional_messages - Required Array of messages for the chat. | 必选 对话的消息数组。
   * @param params.parameters - Required Parameters for the workflow execution. | 必选 工作流执行的参数。
   * @param params.app_id - Optional The ID of the app. | 可选 应用 ID。
   * @param params.bot_id - Optional The ID of the bot. | 可选 Bot ID。
   * @param params.conversation_id - Optional The ID of the conversation. | 可选 会话 ID。
   * @param params.ext - Optional Additional information for the chat. | 可选 对话的附加信息。
   * @returns AsyncGenerator<StreamChatData> | 对话数据流
   */ async *stream(params, options) {
        const apiUrl = '/v1/workflows/chat';
        const payload = {
            ...params,
            additional_messages: handleAdditionalMessages(params.additional_messages)
        };
        const result = await this._client.post(apiUrl, payload, true, options);
        for await (const message of result)if (message.event === chat_ChatEventType.DONE) {
            const ret = {
                event: message.event,
                data: '[DONE]'
            };
            yield ret;
        } else try {
            const ret = {
                event: message.event,
                data: JSON.parse(message.data)
            };
            yield ret;
        } catch (error) {
            throw new CozeError(`Could not parse message into JSON:${message.data}`);
        }
    }
}
class Workflows extends APIResource {
    constructor(...args){
        super(...args), this.runs = new Runs(this._client), this.chat = new WorkflowChat(this._client);
    }
}
class WorkSpaces extends APIResource {
    /**
   * View the list of workspaces that the current Coze user has joined. | 查看当前扣子用户加入的空间列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_workspace?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_workspace?_lang=zh
   * @param params.page_num - Optional The page number for paginated queries. Default is 1.
   * | 可选 分页查询时的页码。默认为 1，即从第一页数据开始返回。
   * @param params.page_size - Optional The size of pagination. Default is 10. Maximum is 50. | 可选 分页大小。默认为 10，最大为 50。
   * @returns OpenSpaceData | 工作空间列表
   */ async list(params, options) {
        const apiUrl = '/v1/workspaces';
        const response = await this._client.get(apiUrl, params, false, options);
        return safeJsonParse(response, response).data;
    }
}
// Required header for knowledge APIs
const documents_headers = {
    'agw-js-conv': 'str'
};
class Documents extends APIResource {
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.list' instead.
   *
   * View the file list of a specified knowledge base, which includes lists of documents, spreadsheets, or images.
   * | 调用接口查看指定知识库的内容列表，即文件、表格或图像列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge base. | 必选 待查看文件的知识库 ID。
   * @param params.page - Optional The page number for paginated queries. Default is 1. | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional The size of pagination. Default is 10. | 可选 分页大小。默认为 10。
   * @returns ListDocumentData | 知识库文件列表
   */ list(params, options) {
        const apiUrl = '/open_api/knowledge/document/list';
        const response = this._client.get(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
        return response;
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.create' instead.
   *
   * Upload files to the specific knowledge. | 调用此接口向指定知识库中上传文件。
   * @docs en: https://www.coze.com/docs/developer_guides/create_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge. | 必选 知识库 ID。
   * @param params.document_bases - Required The metadata information of the files awaiting upload. | 必选 待上传文件的元数据信息。
   * @param params.chunk_strategy - Required when uploading files to a new knowledge for the first time. Chunk strategy.
   * | 向新知识库首次上传文件时必选 分段规则。
   * @returns DocumentInfo[] | 已上传文件的基本信息
   */ async create(params, options) {
        const apiUrl = '/open_api/knowledge/document/create';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
        return response.document_infos;
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.delete' instead.
   *
   * Delete text, images, sheets, and other files in the knowledge base, supporting batch deletion.
   * | 删除知识库中的文本、图像、表格等文件，支持批量删除。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_knowledge_files?_lang=zh
   * @param params.document_ids - Required The list of knowledge base files to be deleted. | 必选 待删除的文件 ID。
   * @returns void | 无返回
   */ async delete(params, options) {
        const apiUrl = '/open_api/knowledge/document/delete';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
    }
    /**
   * @deprecated  The method is deprecated and will be removed in a future version. Please use 'client.datasets.documents.update' instead.
   *
   * Modify the knowledge base file name and update strategy. | 调用接口修改知识库文件名称和更新策略。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_knowledge_files?_lang=zh
   * @param params.document_id - Required The ID of the knowledge base file. | 必选 待修改的知识库文件 ID。
   * @param params.document_name - Optional The new name of the knowledge base file. | 可选 知识库文件的新名称。
   * @param params.update_rule - Optional The update strategy for online web pages. | 可选 在线网页更新策略。
   * @returns void | 无返回
   */ async update(params, options) {
        const apiUrl = '/open_api/knowledge/document/update';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_headers
        }));
    }
}
class Knowledge extends APIResource {
    constructor(...args){
        super(...args), /**
   * @deprecated
   */ this.documents = new Documents(this._client);
    }
}
// Required header for knowledge APIs
const documents_documents_headers = {
    'agw-js-conv': 'str'
};
class documents_Documents extends APIResource {
    /**
   * View the file list of a specified knowledge base, which includes lists of documents, spreadsheets, or images.
   * | 调用接口查看指定知识库的内容列表，即文件、表格或图像列表。
   * @docs en: https://www.coze.com/docs/developer_guides/list_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge base. | 必选 待查看文件的知识库 ID。
   * @param params.page - Optional The page number for paginated queries. Default is 1. | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional The size of pagination. Default is 10. | 可选 分页大小。默认为 10。
   * @returns ListDocumentData | 知识库文件列表
   */ async list(params, options) {
        const apiUrl = '/open_api/knowledge/document/list';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
        return response;
    }
    /**
   * Upload files to the specific knowledge. | 调用此接口向指定知识库中上传文件。
   * @docs en: https://www.coze.com/docs/developer_guides/create_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_knowledge_files?_lang=zh
   * @param params.dataset_id - Required The ID of the knowledge. | 必选 知识库 ID。
   * @param params.document_bases - Required The metadata information of the files awaiting upload. | 必选 待上传文件的元数据信息。
   * @param params.chunk_strategy - Required when uploading files to a new knowledge for the first time. Chunk strategy.
   * | 向新知识库首次上传文件时必选 分段规则。
   * @returns DocumentInfo[] | 已上传文件的基本信息
   */ async create(params, options) {
        const apiUrl = '/open_api/knowledge/document/create';
        const response = await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
        return response.document_infos;
    }
    /**
   * Delete text, images, sheets, and other files in the knowledge base, supporting batch deletion.
   * | 删除知识库中的文本、图像、表格等文件，支持批量删除。
   * @docs en: https://www.coze.com/docs/developer_guides/delete_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_knowledge_files?_lang=zh
   * @param params.document_ids - Required The list of knowledge base files to be deleted. | 必选 待删除的文件 ID。
   * @returns void | 无返回
   */ async delete(params, options) {
        const apiUrl = '/open_api/knowledge/document/delete';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
    }
    /**
   * Modify the knowledge base file name and update strategy. | 调用接口修改知识库文件名称和更新策略。
   * @docs en: https://www.coze.com/docs/developer_guides/modify_knowledge_files?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/modify_knowledge_files?_lang=zh
   * @param params.document_id - Required The ID of the knowledge base file. | 必选 待修改的知识库文件 ID。
   * @param params.document_name - Optional The new name of the knowledge base file. | 可选 知识库文件的新名称。
   * @param params.update_rule - Optional The update strategy for online web pages. | 可选 在线网页更新策略。
   * @returns void | 无返回
   */ async update(params, options) {
        const apiUrl = '/open_api/knowledge/document/update';
        await this._client.post(apiUrl, params, false, mergeConfig(options, {
            headers: documents_documents_headers
        }));
    }
}
class Images extends APIResource {
    /**
   * Update the description of an image in the knowledge base | 更新知识库中的图片描述
   * @docs en: https://www.coze.com/docs/developer_guides/developer_guides/update_image_caption?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/developer_guides/update_image_caption?_lang=zh
   * @param datasetId - The ID of the dataset | 必选 知识库 ID
   * @param documentId - The ID of the document | 必选 知识库文件 ID
   * @param params - The parameters for updating the image
   * @param params.caption - Required. The description of the image | 必选 图片的描述信息
   * @returns undefined
   */ // eslint-disable-next-line max-params
    async update(datasetId, documentId, params, options) {
        const apiUrl = `/v1/datasets/${datasetId}/images/${documentId}`;
        await this._client.put(apiUrl, params, false, options);
    }
    /**
   * List images in the knowledge base | 列出知识库中的图片
   * @docs en: https://www.coze.com/docs/developer_guides/developer_guides/get_images?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/developer_guides/get_images?_lang=zh
   * @param datasetId - The ID of the dataset | 必选 知识库 ID
   * @param params - The parameters for listing images
   * @param params.page_num - Optional. Page number for pagination, minimum value is 1, defaults to 1 | 可选 分页查询时的页码。默认为 1。
   * @param params.page_size - Optional. Number of items per page, range 1-299, defaults to 10 | 可选 分页大小。默认为 10。
   * @param params.keyword - Optional. Search keyword for image descriptions | 可选 图片描述的搜索关键词。
   * @param params.has_caption - Optional. Filter for images with/without captions | 可选 是否过滤有/无描述的图片。
   */ async list(datasetId, params, options) {
        const apiUrl = `/v1/datasets/${datasetId}/images`;
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
}
class Datasets extends APIResource {
    /**
   * Creates a new dataset | 创建数据集
   * @docs en: https://www.coze.com/docs/developer_guides/create_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/create_dataset?_lang=zh
   * @param params - The parameters for creating a dataset
   * @param {string} params.name - Required. Dataset name, maximum length of 100 characters | 必选 数据集名称，最大长度为 100 个字符
   * @param {string} params.space_id - Required. Space ID where the dataset belongs | 必选 数据集所属的空间 ID
   * @param {number} params.format_type - Required. Dataset type (0: Text type, 2: Image type) | 必选 数据集类型 (0: 文本类型, 2: 图片类型)
   * @param {string} [params.description] - Optional. Dataset description | 可选 数据集描述
   * @param {string} [params.file_id] - Optional. Dataset icon file ID from file upload
   */ async create(params, options) {
        const apiUrl = '/v1/datasets';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Lists all datasets in a space | 列出空间中的所有数据集
   * @docs en: https://www.coze.com/docs/developer_guides/list_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/list_dataset?_lang=zh
   * @param params - The parameters for listing datasets | 列出数据集的参数
   * @param {string} params.space_id - Required. Space ID where the datasets belong | 必选 数据集所属的空间 ID
   * @param {string} [params.name] - Optional. Dataset name for fuzzy search | 可选 数据集名称用于模糊搜索
   * @param {number} [params.format_type] - Optional. Dataset type (0: Text type, 2: Image type) | 可选 数据集类型 (0: 文本类型, 2: 图片类型)
   * @param {number} [params.page_num] - Optional. Page number for pagination (default: 1) | 可选 分页查询时的页码。默认为 1。
   * @param {number} [params.page_size] - Optional. Number of items per page (default: 10) | 可选 分页大小。默认为 10。
   */ async list(params, options) {
        const apiUrl = '/v1/datasets';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
    /**
   * Updates a dataset | 更新数据集
   * @docs en: https://www.coze.com/docs/developer_guides/update_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/update_dataset?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to update | 必选 数据集 ID
   * @param params - Required. The parameters for updating the dataset | 必选 更新数据集的参数
   * @param params.name - Required. Dataset name, maximum length of 100 characters. | 必选 数据集名称，最大长度为 100 个字符。
   * @param params.file_id - Optional. Dataset icon, should pass the file_id obtained from the file upload interface. | 可选 数据集图标，应传递从文件上传接口获取的 file_id。
   * @param params.description - Optional. Dataset description. | 可选 数据集描述。
   */ async update(dataset_id, params, options) {
        const apiUrl = `/v1/datasets/${dataset_id}`;
        await this._client.put(apiUrl, params, false, options);
    }
    /**
   * Deletes a dataset | 删除数据集
   * @docs en: https://www.coze.com/docs/developer_guides/delete_dataset?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/delete_dataset?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to delete | 必选 数据集 ID
   */ async delete(dataset_id, options) {
        const apiUrl = `/v1/datasets/${dataset_id}`;
        await this._client.delete(apiUrl, false, options);
    }
    /**
   * Views the progress of dataset upload | 查看数据集上传进度
   * @docs en: https://www.coze.com/docs/developer_guides/get_dataset_progress?_lang=en
   * @docs zh: https://www.coze.cn/docs/developer_guides/get_dataset_progress?_lang=zh
   * @param dataset_id - Required. The ID of the dataset to process | 必选 数据集 ID
   * @param params - Required. The parameters for processing the dataset | 必选 处理数据集的参数
   * @param params.dataset_ids - Required. List of dataset IDs | 必选 数据集 ID 列表
   */ async process(dataset_id, params, options) {
        const apiUrl = `/v1/datasets/${dataset_id}/process`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
    constructor(...args){
        super(...args), this.documents = new documents_Documents(this._client), this.images = new Images(this._client);
    }
}
class Voices extends APIResource {
    /**
   * @description Clone a voice | 音色克隆
   * @param params
   * @param params.voice_name - Required. Voice name, cannot be empty and must be longer than 6 characters
   * | 复刻的音色名称，不能为空，长度大于 6
   * @param params.file - Required. Audio file | 音频文件
   * @param params.audio_format - Required. Only supports "wav", "mp3", "ogg", "m4a", "aac", "pcm" formats
   * | 只支持 "wav", "mp3", "ogg", "m4a", "aac", "pcm" 格式
   * @param params.language - Optional. Only supports "zh", "en" "ja" "es" "id" "pt" languages
   * | 只支持 "zh", "en" "ja" "es" "id" "pt" 语种
   * @param params.voice_id - Optional. If provided, will train on existing voice and override previous training
   * | 传入的话就会在原有的音色上去训练，覆盖前面训练好的音色
   * @param params.preview_text - Optional. If provided, will generate preview audio based on this text, otherwise uses default text
   * | 如果传入会基于该文本生成预览音频，否则使用默认的文本
   * @param params.text - Optional. Users can read this text, service will compare audio with text. Returns error if difference is too large
   * | 可以让用户按照该文本念诵，服务会对比音频与该文本的差异。若差异过大会返回错误
   * @param params.space_id - Optional.  The space id of the voice. | 空间ID
   * @param params.description- Optional. The description of the voice. | 音色描述
   * @param options - Request options
   * @returns Clone voice data
   */ async clone(params, options) {
        const apiUrl = '/v1/audio/voices/clone';
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
    /**
   * @description List voices | 获取音色列表
   * @param params
   * @param params.filter_system_voice - Optional. Whether to filter system voices, default is false
   * | 是否过滤系统音色, 默认不过滤
   * @param params.page_num - Optional. Starts from 1 by default, value must be > 0
   * | 不传默认从 1 开始，传值需要 > 0
   * @param params.page_size - Optional. Default is 100, value must be (0, 100]
   * | 不传默认 100，传值需要 (0, 100]
   * @param options - Request options
   * @returns List voices data
   */ async list(params, options) {
        const apiUrl = '/v1/audio/voices';
        const response = await this._client.get(apiUrl, params, false, options);
        return response.data;
    }
}
class Transcriptions extends APIResource {
    /**
   * ASR voice to text | ASR 语音转文本
   * @param params - Required The parameters for file upload | 上传文件所需的参数
   * @param params.file - Required The audio file to be uploaded. | 需要上传的音频文件。
   */ async create(params, options) {
        const apiUrl = '/v1/audio/transcriptions';
        const response = await this._client.post(apiUrl, (0, __WEBPACK_EXTERNAL_MODULE_axios__.toFormData)(params), false, options);
        return response.data;
    }
}
class Speech extends APIResource {
    /**
   * @description Speech synthesis | 语音合成
   * @param params
   * @param params.input - Required. Text to generate audio | 要为其生成音频的文本
   * @param params.voice_id - Required. Voice ID | 生成音频的音色 ID
   * @param params.response_format - Optional. Audio encoding format,
   * supports "wav", "pcm", "ogg", "opus", "mp3", default is "mp3"
   * | 音频编码格式，支持 "wav", "pcm", "ogg", "opus", "mp3"，默认是 "mp3"
   * @param options - Request options
   * @returns Speech synthesis data
   */ async create(params, options) {
        const apiUrl = '/v1/audio/speech';
        const response = await this._client.post(apiUrl, {
            ...params,
            sample_rate: params.sample_rate || 24000
        }, false, mergeConfig(options, {
            responseType: 'arraybuffer'
        }));
        return response;
    }
}
class Rooms extends APIResource {
    async create(params, options) {
        const apiUrl = '/v1/audio/rooms';
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
}
class Audio extends APIResource {
    constructor(...args){
        super(...args), this.rooms = new Rooms(this._client), this.voices = new Voices(this._client), this.speech = new Speech(this._client), this.transcriptions = new Transcriptions(this._client);
    }
}
class Templates extends APIResource {
    /**
   * Duplicate a template. | 复制一个模板。
   * @param templateId - Required. The ID of the template to duplicate. | 要复制的模板的 ID。
   * @param params - Optional. The parameters for the duplicate operation. | 可选参数，用于复制操作。
   * @param params.workspace_id - Required. The ID of the workspace to duplicate the template into. | 要复制到的目标工作空间的 ID。
   * @param params.name - Optional. The name of the new template. | 新模板的名称。
   * @returns TemplateDuplicateRes | 复制模板结果
   */ async duplicate(templateId, params, options) {
        const apiUrl = `/v1/templates/${templateId}/duplicate`;
        const response = await this._client.post(apiUrl, params, false, options);
        return response.data;
    }
}
class chat_Chat extends APIResource {
    async create(botId, options) {
        const apiUrl = `/v1/chat?bot_id=${botId}`;
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class transcriptions_Transcriptions extends APIResource {
    async create(options) {
        const apiUrl = '/v1/audio/transcriptions';
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class speech_Speech extends APIResource {
    async create(options) {
        const apiUrl = '/v1/audio/speech';
        return await this._client.makeWebsocket(apiUrl, options);
    }
}
class audio_Audio extends APIResource {
    constructor(...args){
        super(...args), this.speech = new speech_Speech(this._client), this.transcriptions = new transcriptions_Transcriptions(this._client);
    }
}
// Common types (not exported)
// Keep all existing exports but use the base types where applicable
var types_WebsocketsEventType = /*#__PURE__*/ function(WebsocketsEventType) {
    // Common
    /** SDK error */ WebsocketsEventType["CLIENT_ERROR"] = "client_error";
    /** Connection closed */ WebsocketsEventType["CLOSED"] = "closed";
    // Error
    /** Received error event */ WebsocketsEventType["ERROR"] = "error";
    // v1/audio/speech
    /** Send text to server */ WebsocketsEventType["INPUT_TEXT_BUFFER_APPEND"] = "input_text_buffer.append";
    /** No text to send, after audio all received, can close connection */ WebsocketsEventType["INPUT_TEXT_BUFFER_COMPLETE"] = "input_text_buffer.complete";
    /** Send speech config to server */ WebsocketsEventType["SPEECH_UPDATE"] = "speech.update";
    /** Received `speech.updated` event */ WebsocketsEventType["SPEECH_UPDATED"] = "speech.updated";
    /** After speech created */ WebsocketsEventType["SPEECH_CREATED"] = "speech.created";
    /** Received `input_text_buffer.complete` event */ WebsocketsEventType["INPUT_TEXT_BUFFER_COMPLETED"] = "input_text_buffer.completed";
    /** Received `speech.update` event */ WebsocketsEventType["SPEECH_AUDIO_UPDATE"] = "speech.audio.update";
    /** All audio received, can close connection */ WebsocketsEventType["SPEECH_AUDIO_COMPLETED"] = "speech.audio.completed";
    // v1/audio/transcriptions
    /** Send audio to server */ WebsocketsEventType["INPUT_AUDIO_BUFFER_APPEND"] = "input_audio_buffer.append";
    /** No audio to send, after text all received, can close connection */ WebsocketsEventType["INPUT_AUDIO_BUFFER_COMPLETE"] = "input_audio_buffer.complete";
    /** Send transcriptions config to server */ WebsocketsEventType["TRANSCRIPTIONS_UPDATE"] = "transcriptions.update";
    /** Send `input_audio_buffer.clear` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_CLEAR"] = "input_audio_buffer.clear";
    /** After transcriptions created */ WebsocketsEventType["TRANSCRIPTIONS_CREATED"] = "transcriptions.created";
    /** Received `input_audio_buffer.complete` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_COMPLETED"] = "input_audio_buffer.completed";
    /** Received `transcriptions.update` event */ WebsocketsEventType["TRANSCRIPTIONS_MESSAGE_UPDATE"] = "transcriptions.message.update";
    /** All audio received, can close connection */ WebsocketsEventType["TRANSCRIPTIONS_MESSAGE_COMPLETED"] = "transcriptions.message.completed";
    /** Received `input_audio_buffer.cleared` event */ WebsocketsEventType["INPUT_AUDIO_BUFFER_CLEARED"] = "input_audio_buffer.cleared";
    /** Received `transcriptions.updated` event */ WebsocketsEventType["TRANSCRIPTIONS_UPDATED"] = "transcriptions.updated";
    // v1/chat
    /** Send chat config to server */ WebsocketsEventType["CHAT_UPDATE"] = "chat.update";
    /** Send tool outputs to server */ WebsocketsEventType["CONVERSATION_CHAT_SUBMIT_TOOL_OUTPUTS"] = "conversation.chat.submit_tool_outputs";
    /** After chat created */ WebsocketsEventType["CHAT_CREATED"] = "chat.created";
    /** After chat updated */ WebsocketsEventType["CHAT_UPDATED"] = "chat.updated";
    /** Audio AST completed, chat started */ WebsocketsEventType["CONVERSATION_CHAT_CREATED"] = "conversation.chat.created";
    /** Message created */ WebsocketsEventType["CONVERSATION_MESSAGE_CREATE"] = "conversation.message.create";
    /** Clear conversation */ WebsocketsEventType["CONVERSATION_CLEAR"] = "conversation.clear";
    /** Chat in progress */ WebsocketsEventType["CONVERSATION_CHAT_IN_PROGRESS"] = "conversation.chat.in_progress";
    /** Get agent text message update */ WebsocketsEventType["CONVERSATION_MESSAGE_DELTA"] = "conversation.message.delta";
    /** Need plugin submit */ WebsocketsEventType["CONVERSATION_CHAT_REQUIRES_ACTION"] = "conversation.chat.requires_action";
    /** Message completed */ WebsocketsEventType["CONVERSATION_MESSAGE_COMPLETED"] = "conversation.message.completed";
    /** Get agent audio message update */ WebsocketsEventType["CONVERSATION_AUDIO_DELTA"] = "conversation.audio.delta";
    /** Audio message completed */ WebsocketsEventType["CONVERSATION_AUDIO_COMPLETED"] = "conversation.audio.completed";
    /** All message received, can close connection */ WebsocketsEventType["CONVERSATION_CHAT_COMPLETED"] = "conversation.chat.completed";
    /** Chat failed */ WebsocketsEventType["CONVERSATION_CHAT_FAILED"] = "conversation.chat.failed";
    /** Received `conversation.cleared` event */ WebsocketsEventType["CONVERSATION_CLEARED"] = "conversation.cleared";
    return WebsocketsEventType;
}({});
class Websockets extends APIResource {
    constructor(...args){
        super(...args), this.audio = new audio_Audio(this._client), this.chat = new chat_Chat(this._client);
    }
}
class WebSocketAPI {
    // Standard WebSocket properties
    get readyState() {
        return this.rws.readyState;
    }
    // Standard WebSocket methods
    send(data) {
        return this.rws.send(JSON.stringify(data));
    }
    close(code, reason) {
        return this.rws.close(code, reason);
    }
    reconnect(code, reason) {
        return this.rws.reconnect(code, reason);
    }
    // Event listener methods
    addEventListener(type, listener) {
        this.rws.addEventListener(type, listener);
    }
    removeEventListener(type, listener) {
        this.rws.removeEventListener(type, listener);
    }
    constructor(url, options = {}){
        // Event handler methods
        this.onmessage = null;
        this.onopen = null;
        this.onclose = null;
        this.onerror = null;
        const separator = url.includes('?') ? '&' : '?';
        const { authorization } = options.headers || {};
        this.rws = new __WEBPACK_EXTERNAL_MODULE_reconnecting_websocket__["default"](`${url}${separator}authorization=${authorization}`, [], {
            WebSocket: utils_isBrowser() ? window.WebSocket : class extends __WEBPACK_EXTERNAL_MODULE_ws__["default"] {
                constructor(url2, protocols){
                    super(url2, protocols, {
                        headers: options.headers
                    });
                }
            },
            ...options
        });
        this.rws.addEventListener('message', (event)=>{
            try {
                var _this_onmessage, _this;
                const data = JSON.parse(event.data);
                null === (_this_onmessage = (_this = this).onmessage) || void 0 === _this_onmessage || _this_onmessage.call(_this, data, event);
            } catch (error) {
                console.error('WebSocketAPI onmessage error', error);
            }
        });
        this.rws.addEventListener('open', (event)=>{
            var _this_onopen, _this;
            null === (_this_onopen = (_this = this).onopen) || void 0 === _this_onopen || _this_onopen.call(_this, event);
        });
        this.rws.addEventListener('close', (event)=>{
            var _this_onclose, _this;
            null === (_this_onclose = (_this = this).onclose) || void 0 === _this_onclose || _this_onclose.call(_this, event);
        });
        this.rws.addEventListener('error', (event)=>{
            var _event_target__req_res, _event_target__req, _event_target, _event_target__req_res1, _event_target__req1, _event_target1, _this_onerror, _this;
            const { readyState } = this.rws;
            if (3 === readyState) return;
            const statusCode = null === (_event_target = event.target) || void 0 === _event_target ? void 0 : null === (_event_target__req = _event_target._req) || void 0 === _event_target__req ? void 0 : null === (_event_target__req_res = _event_target__req.res) || void 0 === _event_target__req_res ? void 0 : _event_target__req_res.statusCode;
            const rawHeaders = (null === (_event_target1 = event.target) || void 0 === _event_target1 ? void 0 : null === (_event_target__req1 = _event_target1._req) || void 0 === _event_target__req1 ? void 0 : null === (_event_target__req_res1 = _event_target__req1.res) || void 0 === _event_target__req_res1 ? void 0 : _event_target__req_res1.rawHeaders) || [];
            const logidIndex = rawHeaders.findIndex((header)=>'X-Tt-Logid' === header);
            const logid = -1 !== logidIndex ? rawHeaders[logidIndex + 1] : void 0;
            const error = {
                id: '0',
                event_type: types_WebsocketsEventType.ERROR,
                data: {
                    code: -1,
                    msg: 'WebSocket error'
                },
                detail: {
                    logid
                }
            };
            if (401 === statusCode) {
                error.data.code = 401;
                error.data.msg = 'Unauthorized';
            } else if (403 === statusCode) {
                error.data.code = 403;
                error.data.msg = 'Forbidden';
            } else {
                error.data.code = 500;
                var _event_error;
                error.data.msg = String(null !== (_event_error = null == event ? void 0 : event.error) && void 0 !== _event_error ? _event_error : '') || 'WebSocket error';
            }
            null === (_this_onerror = (_this = this).onerror) || void 0 === _this_onerror || _this_onerror.call(_this, error, event);
        });
    }
}
var package_namespaceObject = JSON.parse('{"name":"@coze/api","version":"1.1.1","description":"Official Coze Node.js SDK for seamless AI integration into your applications | 扣子官方 Node.js SDK，助您轻松集成 AI 能力到应用中","keywords":["coze","ai","nodejs","sdk","chatbot","typescript"],"homepage":"https://github.com/coze-dev/coze-js/tree/main/packages/coze-js","bugs":{"url":"https://github.com/coze-dev/coze-js/issues"},"repository":{"type":"git","url":"https://github.com/coze-dev/coze-js.git","directory":"packages/coze-js"},"license":"MIT","author":"Leeight <leeight@gmail.com>","exports":{".":"./src/index.ts","./ws-tools":"./src/ws-tools/index.ts"},"main":"src/index.ts","module":"src/index.ts","browser":{"crypto":false,"os":false,"jsonwebtoken":false,"node-fetch":false},"typesVersions":{"*":{".":["dist/types/index.d.ts"],"ws-tools":["dist/types/ws-tools/ws-tools/index.d.ts"]}},"files":["dist","LICENSE","README.md","README.zh-CN.md"],"scripts":{"build":"rslib build","format":"prettier --write .","lint":"eslint ./ --cache --quiet","start":"rslib build -w","test":"vitest","test:cov":"vitest --coverage --run"},"dependencies":{"jsonwebtoken":"^9.0.2","node-fetch":"^2.x","reconnecting-websocket":"^4.4.0","uuid":"^10.0.0","ws":"^8.11.0"},"devDependencies":{"@coze-infra/eslint-config":"workspace:*","@coze-infra/ts-config":"workspace:*","@coze-infra/vitest-config":"workspace:*","@rslib/core":"0.0.18","@swc/core":"^1.3.14","@types/jsonwebtoken":"^9.0.0","@types/node":"^20","@types/node-fetch":"^2.x","@types/uuid":"^9.0.1","@types/whatwg-fetch":"^0.0.33","@types/ws":"^8.5.1","@vitest/coverage-v8":"~2.1.4","axios":"^1.7.7","typescript":"^5.5.3","vitest":"~2.1.4"},"peerDependencies":{"axios":"^1.7.1"},"cozePublishConfig":{"exports":{".":{"require":"./dist/cjs/index.js","import":"./dist/esm/index.js","types":"./dist/types/index.d.ts"},"./ws-tools":{"require":"./dist/cjs/ws-tools/index.js","import":"./dist/esm/ws-tools/index.js","types":"./dist/types/ws-tools/ws-tools/index.d.ts"}},"main":"dist/cjs/index.js","module":"dist/esm/index.js","types":"dist/types/index.d.ts"}}'); // CONCATENATED MODULE: ./src/version.ts
const { version: version_version } = package_namespaceObject;
const getEnv = ()=>{
    const nodeVersion = process.version.slice(1); // Remove 'v' prefix
    const { platform } = process;
    let osName = platform.toLowerCase();
    let osVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release();
    if ('darwin' === platform) {
        osName = 'macos';
        // Try to parse the macOS version
        try {
            const darwinVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release().split('.');
            if (darwinVersion.length >= 2) {
                const majorVersion = parseInt(darwinVersion[0], 10);
                if (!isNaN(majorVersion) && majorVersion >= 9) {
                    const macVersion = majorVersion - 9;
                    osVersion = `10.${macVersion}.${darwinVersion[1]}`;
                }
            }
        } catch (error) {
        // Keep the default os.release() value if parsing fails
        }
    } else if ('win32' === platform) {
        osName = 'windows';
        osVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release();
    } else if ('linux' === platform) {
        osName = 'linux';
        osVersion = __WEBPACK_EXTERNAL_MODULE_os__["default"].release();
    }
    return {
        osName,
        osVersion,
        nodeVersion
    };
};
const getUserAgent = ()=>{
    const { nodeVersion, osName, osVersion } = getEnv();
    return `coze-js/${version_version} node/${nodeVersion} ${osName}/${osVersion}`.toLowerCase();
};
const getNodeClientUserAgent = ()=>{
    const { osVersion, nodeVersion, osName } = getEnv();
    const ua = {
        version: version_version,
        lang: 'node',
        lang_version: nodeVersion,
        os_name: osName,
        os_version: osVersion
    };
    return JSON.stringify(ua);
};
const getBrowserClientUserAgent = ()=>{
    const browserInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const osInfo = {
        name: 'unknown',
        version: 'unknown'
    };
    const { userAgent } = navigator;
    // 检测操作系统及版本
    if (userAgent.indexOf('Windows') > -1) {
        var _userAgent_match;
        osInfo.name = 'windows';
        const windowsVersion = (null === (_userAgent_match = userAgent.match(/Windows NT ([0-9.]+)/)) || void 0 === _userAgent_match ? void 0 : _userAgent_match[1]) || 'unknown';
        osInfo.version = windowsVersion;
    } else if (userAgent.indexOf('Mac OS X') > -1) {
        var _userAgent_match1;
        osInfo.name = 'macos';
        // 将 10_15_7 格式转换为 10.15.7
        osInfo.version = ((null === (_userAgent_match1 = userAgent.match(/Mac OS X ([0-9_]+)/)) || void 0 === _userAgent_match1 ? void 0 : _userAgent_match1[1]) || 'unknown').replace(/_/g, '.');
    } else if (userAgent.indexOf('Linux') > -1) {
        var _userAgent_match2;
        osInfo.name = 'linux';
        osInfo.version = (null === (_userAgent_match2 = userAgent.match(/Linux ([0-9.]+)/)) || void 0 === _userAgent_match2 ? void 0 : _userAgent_match2[1]) || 'unknown';
    }
    // 检测浏览器及版本
    if (userAgent.indexOf('Chrome') > -1) {
        var _userAgent_match3;
        browserInfo.name = 'chrome';
        browserInfo.version = (null === (_userAgent_match3 = userAgent.match(/Chrome\/([0-9.]+)/)) || void 0 === _userAgent_match3 ? void 0 : _userAgent_match3[1]) || 'unknown';
    } else if (userAgent.indexOf('Firefox') > -1) {
        var _userAgent_match4;
        browserInfo.name = 'firefox';
        browserInfo.version = (null === (_userAgent_match4 = userAgent.match(/Firefox\/([0-9.]+)/)) || void 0 === _userAgent_match4 ? void 0 : _userAgent_match4[1]) || 'unknown';
    } else if (userAgent.indexOf('Safari') > -1) {
        var _userAgent_match5;
        browserInfo.name = 'safari';
        browserInfo.version = (null === (_userAgent_match5 = userAgent.match(/Version\/([0-9.]+)/)) || void 0 === _userAgent_match5 ? void 0 : _userAgent_match5[1]) || 'unknown';
    }
    const ua = {
        version: version_version,
        browser: browserInfo.name,
        browser_version: browserInfo.version,
        os_name: osInfo.name,
        os_version: osInfo.version
    };
    return JSON.stringify(ua);
};
/* eslint-disable @typescript-eslint/no-explicit-any */ const handleError = (error)=>{
    if (!error.isAxiosError && (!error.code || !error.message)) return new CozeError(`Unexpected error: ${error.message}`);
    if ('ECONNABORTED' === error.code && error.message.includes('timeout') || 'ETIMEDOUT' === error.code) {
        var _error_response;
        return new TimeoutError(408, void 0, `Request timed out: ${error.message}`, null === (_error_response = error.response) || void 0 === _error_response ? void 0 : _error_response.headers);
    }
    if ('ERR_CANCELED' === error.code) return new APIUserAbortError(error.message);
    else {
        var _error_response1, _error_response2, _error_response3;
        return error_APIError.generate((null === (_error_response1 = error.response) || void 0 === _error_response1 ? void 0 : _error_response1.status) || 500, null === (_error_response2 = error.response) || void 0 === _error_response2 ? void 0 : _error_response2.data, error.message, null === (_error_response3 = error.response) || void 0 === _error_response3 ? void 0 : _error_response3.headers);
    }
};
// node-fetch is used for streaming requests
const adapterFetch = async (options)=>{
    const response = await (0, __WEBPACK_EXTERNAL_MODULE_node_fetch__["default"])(options.url, {
        body: options.data,
        ...options
    });
    return {
        data: response.body,
        ...response
    };
};
const isSupportNativeFetch = ()=>{
    if (utils_isBrowser()) return true;
    // native fetch is supported in node 18.0.0 or higher
    const version = process.version.slice(1);
    return compareVersions(version, '18.0.0') >= 0;
};
async function fetchAPI(url) {
    let options = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {};
    const axiosInstance = options.axiosInstance || __WEBPACK_EXTERNAL_MODULE_axios__["default"];
    // Add version check for streaming requests
    if (options.isStreaming && isAxiosStatic(axiosInstance)) {
        const axiosVersion = axiosInstance.VERSION || __WEBPACK_EXTERNAL_MODULE_axios__["default"].VERSION;
        if (!axiosVersion || compareVersions(axiosVersion, '1.7.1') < 0) throw new CozeError('Streaming requests require axios version 1.7.1 or higher. Please upgrade your axios version.');
    }
    const response = await axiosInstance({
        url,
        responseType: options.isStreaming ? 'stream' : 'json',
        adapter: options.isStreaming ? isSupportNativeFetch() ? 'fetch' : adapterFetch : void 0,
        ...options
    }).catch((error)=>{
        throw handleError(error);
    });
    return {
        async *stream () {
            try {
                const stream = response.data;
                const reader = stream[Symbol.asyncIterator] ? stream[Symbol.asyncIterator]() : stream.getReader();
                const decoder = new TextDecoder();
                const fieldValues = {};
                let buffer = '';
                while(true){
                    const { done, value } = await (reader.next ? reader.next() : reader.read());
                    if (done) {
                        if (buffer) {
                            // If the stream ends without a newline, it means an error occurred
                            fieldValues.event = 'error';
                            fieldValues.data = buffer;
                            yield fieldValues;
                        }
                        break;
                    }
                    buffer += decoder.decode(value, {
                        stream: true
                    });
                    const lines = buffer.split('\n');
                    for(let i = 0; i < lines.length - 1; i++){
                        const line = lines[i];
                        const index = line.indexOf(':');
                        if (-1 !== index) {
                            const field = line.substring(0, index).trim();
                            const content = line.substring(index + 1).trim();
                            fieldValues[field] = content;
                            if ('data' === field) yield fieldValues;
                        }
                    }
                    buffer = lines[lines.length - 1]; // Keep the last incomplete line in the buffer
                }
            } catch (error) {
                handleError(error);
            }
        },
        json: ()=>response.data,
        response
    };
}
// Add version comparison utility
function compareVersions(v1, v2) {
    const v1Parts = v1.split('.').map(Number);
    const v2Parts = v2.split('.').map(Number);
    for(let i = 0; i < 3; i++){
        const part1 = v1Parts[i] || 0;
        const part2 = v2Parts[i] || 0;
        if (part1 > part2) return 1;
        if (part1 < part2) return -1;
    }
    return 0;
}
function isAxiosStatic(instance) {
    return !!(null == instance ? void 0 : instance.Axios);
}
/**
 * default coze  base URL is api.coze.com
 */ const constant_COZE_COM_BASE_URL = 'https://api.coze.com';
/**
 * default base websocket URL is wss://ws.coze.com
 */ const COZE_COM_BASE_WS_URL = 'wss://ws.coze.com';
/* eslint-disable max-params */ class core_APIClient {
    async getToken() {
        if ('function' == typeof this.token) return await this.token();
        return this.token;
    }
    async buildOptions(method, body, options) {
        const token = await this.getToken();
        const headers = {
            authorization: `Bearer ${token}`
        };
        if (utils_isBrowser()) headers['X-Coze-Client-User-Agent'] = getBrowserClientUserAgent();
        else {
            headers['User-Agent'] = getUserAgent();
            headers['X-Coze-Client-User-Agent'] = getNodeClientUserAgent();
        }
        const config = mergeConfig(this.axiosOptions, options, {
            headers
        }, {
            headers: this.headers || {}
        });
        config.method = method;
        config.data = body;
        return config;
    }
    async buildWebsocketOptions(options) {
        const token = await this.getToken();
        const headers = {
            authorization: `Bearer ${token}`
        };
        if (utils_isBrowser()) headers['X-Coze-Client-User-Agent'] = getBrowserClientUserAgent();
        else {
            headers['User-Agent'] = getUserAgent();
            headers['X-Coze-Client-User-Agent'] = getNodeClientUserAgent();
        }
        var _this__config_debug;
        const config = mergeConfig({
            debug: null !== (_this__config_debug = this._config.debug) && void 0 !== _this__config_debug && _this__config_debug
        }, this._config.websocketOptions, options, {
            headers
        }, {
            headers: this.headers || {}
        });
        return config;
    }
    async makeRequest(apiUrl, method, body, isStream, options) {
        const fullUrl = `${this.baseURL}${apiUrl}`;
        const fetchOptions = await this.buildOptions(method, body, options);
        fetchOptions.isStreaming = isStream;
        fetchOptions.axiosInstance = this.axiosInstance;
        this.debugLog(null == options ? void 0 : options.debug, `--- request url: ${fullUrl}`);
        this.debugLog(null == options ? void 0 : options.debug, '--- request options:', fetchOptions);
        const { response, stream, json } = await fetchAPI(fullUrl, fetchOptions);
        this.debugLog(null == options ? void 0 : options.debug, `--- response status: ${response.status}`);
        this.debugLog(null == options ? void 0 : options.debug, '--- response headers: ', response.headers);
        var _response_headers;
        // Taro use `header`
        const contentType = (null !== (_response_headers = response.headers) && void 0 !== _response_headers ? _response_headers : response.header)['content-type'];
        if (isStream) {
            if (contentType && contentType.includes('application/json')) {
                const result = await json();
                const { code, msg } = result;
                if (0 !== code && void 0 !== code) throw error_APIError.generate(response.status, result, msg, response.headers);
            }
            return stream();
        }
        if (!(contentType && contentType.includes('application/json'))) return await response.data;
        {
            const result = await json();
            const { code, msg } = result;
            if (0 !== code && void 0 !== code) throw error_APIError.generate(response.status, result, msg, response.headers);
            return result;
        }
    }
    async post(apiUrl, body) {
        let isStream = arguments.length > 2 && void 0 !== arguments[2] && arguments[2], options = arguments.length > 3 ? arguments[3] : void 0;
        return this.makeRequest(apiUrl, 'POST', body, isStream, options);
    }
    async get(apiUrl, param, isStream, options) {
        // 拼接参数
        const queryString = Object.entries(param || {}).map((param)=>{
            let [key, value] = param;
            return `${key}=${value}`;
        }).join('&');
        return this.makeRequest(queryString ? `${apiUrl}${apiUrl.includes('?') ? '&' : '?'}${queryString}` : apiUrl, 'GET', void 0, isStream, options);
    }
    async put(apiUrl, body, isStream, options) {
        return this.makeRequest(apiUrl, 'PUT', body, isStream, options);
    }
    async delete(apiUrl, isStream, options) {
        return this.makeRequest(apiUrl, 'DELETE', void 0, isStream, options);
    }
    async makeWebsocket(apiUrl, options) {
        const fullUrl = `${this.baseWsURL}${apiUrl}`;
        const websocketOptions = await this.buildWebsocketOptions(options);
        this.debugLog(null == options ? void 0 : options.debug, `--- websocket url: ${fullUrl}`);
        this.debugLog(null == options ? void 0 : options.debug, '--- websocket options:', websocketOptions);
        const ws = new WebSocketAPI(fullUrl, websocketOptions);
        return ws;
    }
    getConfig() {
        return this._config;
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    debugLog() {
        let forceDebug = arguments.length > 0 && void 0 !== arguments[0] && arguments[0];
        for(var _len = arguments.length, msgs = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++)msgs[_key - 1] = arguments[_key];
        if (this.debug || forceDebug) console.debug(...msgs);
    }
    constructor(config){
        this._config = config;
        this.baseURL = config.baseURL || constant_COZE_COM_BASE_URL;
        this.baseWsURL = config.baseWsURL || COZE_COM_BASE_WS_URL;
        this.token = config.token;
        this.axiosOptions = config.axiosOptions || {};
        this.axiosInstance = config.axiosInstance;
        this.debug = config.debug || false;
        this.allowPersonalAccessTokenInBrowser = config.allowPersonalAccessTokenInBrowser || false;
        this.headers = config.headers;
        if (utils_isBrowser() && 'function' != typeof this.token && isPersonalAccessToken(this.token) && !this.allowPersonalAccessTokenInBrowser) throw new CozeError('Browser environments do not support authentication using Personal Access Token (PAT) by default.\nas it may expose secret API keys. \n\nPlease use OAuth2.0 authentication mechanism. see:\nhttps://www.coze.com/docs/developer_guides/oauth_apps?_lang=en \n\nIf you need to force use, please set the `allowPersonalAccessTokenInBrowser` option to `true`. \n\ne.g new CozeAPI({ token, allowPersonalAccessTokenInBrowser: true });\n\n');
    }
}
core_APIClient.APIError = error_APIError;
core_APIClient.BadRequestError = BadRequestError;
core_APIClient.AuthenticationError = AuthenticationError;
core_APIClient.PermissionDeniedError = PermissionDeniedError;
core_APIClient.NotFoundError = NotFoundError;
core_APIClient.RateLimitError = RateLimitError;
core_APIClient.InternalServerError = InternalServerError;
core_APIClient.GatewayError = GatewayError;
core_APIClient.TimeoutError = TimeoutError;
core_APIClient.UserAbortError = APIUserAbortError;
class CozeAPI extends core_APIClient {
    constructor(...args){
        super(...args), this.bots = new Bots(this), this.chat = new Chat(this), this.conversations = new Conversations(this), this.files = new Files(this), /**
   * @deprecated
   */ this.knowledge = new Knowledge(this), this.datasets = new Datasets(this), this.workflows = new Workflows(this), this.workspaces = new WorkSpaces(this), this.audio = new Audio(this), this.templates = new Templates(this), this.websockets = new Websockets(this);
    }
}
class WsSpeechClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.audio.speech.create();
        this.ws = ws;
        let isResolved = false;
        this.trackId = `my-track-id-${(0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)()}`;
        this.totalDuration = 0;
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        this.playbackStartTime = null;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                console.debug('[speech] ws open');
            };
            ws.onmessage = (data, event)=>{
                // Trigger all registered event listeners
                this.emit('data', data);
                this.emit(data.event_type, data);
                if (data.event_type === types_WebsocketsEventType.ERROR) {
                    this.closeWs();
                    if (isResolved) return;
                    isResolved = true;
                    reject(new error_APIError(data.data.code, data, data.data.msg, void 0));
                    return;
                }
                if (data.event_type === types_WebsocketsEventType.SPEECH_CREATED) {
                    resolve(ws);
                    isResolved = true;
                } else if (data.event_type === types_WebsocketsEventType.SPEECH_AUDIO_UPDATE) this.handleAudioMessage(data.data.delta);
                else if (data.event_type === types_WebsocketsEventType.SPEECH_AUDIO_COMPLETED) {
                    console.debug('[speech] totalDuration', this.totalDuration);
                    if (this.playbackStartTime) {
                        // 剩余时间 = 总时间 - 已播放时间 - 已暂停时间
                        const now = new Date().getTime();
                        const remaining = this.totalDuration - (now - this.playbackStartTime) / 1000 - this.elapsedBeforePause;
                        this.playbackTimeout = setTimeout(()=>{
                            this.emit('completed', void 0);
                            this.playbackStartTime = null;
                            this.elapsedBeforePause = 0;
                        }, 1000 * remaining);
                    }
                    this.closeWs();
                }
            };
            ws.onerror = (error, event)=>{
                console.error('[speech] WebSocket error', error, event);
                this.emit('data', error);
                this.emit(types_WebsocketsEventType.ERROR, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                console.debug('[speech] ws close');
            };
        });
    }
    async connect() {
        let { voiceId } = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
        var _this_ws;
        await this.init();
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.SPEECH_UPDATE,
            data: {
                output_audio: {
                    codec: 'pcm',
                    voice_id: voiceId || void 0
                }
            }
        });
    }
    async disconnect() {
        if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
        await this.wavStreamPlayer.interrupt();
        this.closeWs();
    }
    append(message) {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.INPUT_TEXT_BUFFER_APPEND,
            data: {
                delta: message
            }
        });
    }
    complete() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.INPUT_TEXT_BUFFER_COMPLETE
        });
    }
    appendAndComplete(message) {
        this.append(message);
        this.complete();
    }
    async interrupt() {
        await this.wavStreamPlayer.interrupt();
        this.trackId = `my-track-id-${(0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)()}`;
        this.emit('completed', void 0);
        console.debug('[speech] playback completed', this.totalDuration);
    }
    async pause() {
        if (this.playbackTimeout) {
            clearTimeout(this.playbackTimeout);
            this.playbackTimeout = null;
        }
        if (this.playbackStartTime && !this.playbackPauseTime) {
            this.playbackPauseTime = Date.now();
            this.elapsedBeforePause += (this.playbackPauseTime - this.playbackStartTime) / 1000;
        }
        await this.wavStreamPlayer.pause();
    }
    async resume() {
        if (this.playbackPauseTime) {
            this.playbackStartTime = Date.now();
            this.playbackPauseTime = null;
            // Update the timeout with remaining duration
            if (this.playbackTimeout) clearTimeout(this.playbackTimeout);
            const remaining = this.totalDuration - this.elapsedBeforePause;
            this.playbackTimeout = setTimeout(()=>{
                this.emit('completed', void 0);
                console.debug('[speech] playback completed', this.totalDuration);
                this.playbackStartTime = null;
                this.elapsedBeforePause = 0;
            }, 1000 * remaining);
        }
        await this.wavStreamPlayer.resume();
    }
    async togglePlay() {
        if (this.isPlaying()) await this.pause();
        else await this.resume();
    }
    isPlaying() {
        return this.wavStreamPlayer.isPlaying();
    }
    on(event, callback) {
        var _this_listeners_get;
        if (!this.listeners.has(event)) this.listeners.set(event, new Set());
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
    }
    off(event, callback) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    emit(event, data) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(data));
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.trackId = 'default';
        this.totalDuration = 0;
        this.playbackStartTime = null;
        this.playbackPauseTime = null;
        this.playbackTimeout = null;
        this.elapsedBeforePause = 0;
        this.handleAudioMessage = async (message)=>{
            const decodedContent = atob(message);
            const arrayBuffer = new ArrayBuffer(decodedContent.length);
            const view = new Uint8Array(arrayBuffer);
            for(let i = 0; i < decodedContent.length; i++)view[i] = decodedContent.charCodeAt(i);
            // Calculate duration in seconds
            const bytesPerSecond = 48000; // sampleRate * channels * (bitDepth/8)
            const duration = arrayBuffer.byteLength / bytesPerSecond;
            this.totalDuration += duration;
            try {
                await this.wavStreamPlayer.add16BitPCM(arrayBuffer, this.trackId);
                // Start or update the playback timer
                if (!this.playbackStartTime && !this.playbackPauseTime) {
                    this.playbackStartTime = Date.now();
                    this.elapsedBeforePause = 0;
                }
            } catch (error) {
                console.warn('[speech] wavStreamPlayer error', error);
            }
        };
        this.api = new CozeAPI({
            ...config
        });
        this.wavStreamPlayer = new WavStreamPlayer({
            sampleRate: 24000
        });
    }
}
/* ESM default export */ const speech = WsSpeechClient;
class WsTranscriptionClient {
    async init() {
        if (this.ws) return this.ws;
        const ws = await this.api.websockets.audio.transcriptions.create();
        let isResolved = false;
        return new Promise((resolve, reject)=>{
            ws.onopen = ()=>{
                console.debug('[transcription] ws open');
            };
            ws.onmessage = (data, event)=>{
                // Trigger all registered event listeners
                this.emit('data', data);
                this.emit(data.event_type, data);
                if (data.event_type === types_WebsocketsEventType.ERROR) {
                    this.closeWs();
                    if (isResolved) return;
                    isResolved = true;
                    reject(new error_APIError(data.data.code, data, data.data.msg, void 0));
                    return;
                }
                if (data.event_type === types_WebsocketsEventType.TRANSCRIPTIONS_CREATED) {
                    resolve(ws);
                    isResolved = true;
                } else if (data.event_type === types_WebsocketsEventType.TRANSCRIPTIONS_MESSAGE_COMPLETED) this.closeWs();
            };
            ws.onerror = (error, event)=>{
                console.error('[transcription] WebSocket error', error, event);
                this.emit('data', error);
                this.emit(types_WebsocketsEventType.ERROR, error);
                this.closeWs();
                if (isResolved) return;
                isResolved = true;
                reject(new error_APIError(error.data.code, error, error.data.msg, void 0));
            };
            ws.onclose = ()=>{
                console.debug('[transcription] ws close');
            };
            this.ws = ws;
        });
    }
    async connect() {
        var _this_ws;
        await this.init();
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.TRANSCRIPTIONS_UPDATE,
            data: {
                input_audio: {
                    format: 'pcm',
                    codec: 'pcm',
                    sample_rate: 24000,
                    channel: 1,
                    bit_depth: 16
                }
            }
        });
    }
    async disconnect() {
        await this.wavRecorder.quit();
        this.listeners.clear();
        this.closeWs();
    }
    getDeviceList() {
        return this.wavRecorder.listDevices();
    }
    getStatus() {
        return this.wavRecorder.getStatus();
    }
    async record() {
        if ('recording' === this.getStatus()) return;
        if ('ended' === this.getStatus()) {
            var _deviceList_;
            const deviceList = await this.getDeviceList();
            await this.wavRecorder.begin(null === (_deviceList_ = deviceList[0]) || void 0 === _deviceList_ ? void 0 : _deviceList_.deviceId);
        }
        await this.wavRecorder.record((data)=>{
            var _this_ws;
            const { raw } = data;
            // Convert ArrayBuffer to base64 string
            const base64String = btoa(Array.from(new Uint8Array(raw)).map((byte)=>String.fromCharCode(byte)).join(''));
            null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
                id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
                event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_APPEND,
                data: {
                    delta: base64String
                }
            });
        });
    }
    async end() {
        var _this_ws;
        null === (_this_ws = this.ws) || void 0 === _this_ws || _this_ws.send({
            id: (0, __WEBPACK_EXTERNAL_MODULE_uuid__.v4)(),
            event_type: types_WebsocketsEventType.INPUT_AUDIO_BUFFER_COMPLETE
        });
        await this.wavRecorder.pause();
        const finalAudio = await this.wavRecorder.end();
        return finalAudio;
    }
    pause() {
        return this.wavRecorder.pause();
    }
    on(event, callback) {
        var _this_listeners_get;
        if (!this.listeners.has(event)) this.listeners.set(event, new Set());
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.add(callback);
    }
    off(event, callback) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.delete(callback);
    }
    closeWs() {
        var _this_ws;
        if ((null === (_this_ws = this.ws) || void 0 === _this_ws ? void 0 : _this_ws.readyState) === 1) {
            var _this_ws1;
            null === (_this_ws1 = this.ws) || void 0 === _this_ws1 || _this_ws1.close();
        }
        this.ws = null;
    }
    emit(event, data) {
        var _this_listeners_get;
        null === (_this_listeners_get = this.listeners.get(event)) || void 0 === _this_listeners_get || _this_listeners_get.forEach((callback)=>callback(data));
    }
    constructor(config){
        this.ws = null;
        this.listeners = new Map();
        this.api = new CozeAPI({
            ...config
        });
        this.wavRecorder = new WavRecorder({
            sampleRate: 24000
        });
    }
}
/* ESM default export */ const transcription = WsTranscriptionClient;
export { speech as WsSpeechClient, transcription as WsTranscriptionClient };
